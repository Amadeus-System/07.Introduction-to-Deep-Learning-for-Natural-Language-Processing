{"cells":[{"cell_type":"markdown","metadata":{"id":"tGDdhXCXOpNH"},"source":["## 4) 양방향 LSTM과 어텐션 메커니즘(BiLSTM with Attention mechanism)\r\n","\r\n","단방향 LSTM으로 텍스트 분류를 수행할 수도 있지만 때로는 양방향 LSTM을 사용하는 것이 더 강력하다. 여기에 추가적으로 어텐션 메커니즘을 사용할 수도 있다. 양방향 LSTM과 어텐션 메커니즘으로 IMDB 리뷰 감성 분류하기를 수행해보자.\r\n"]},{"cell_type":"markdown","metadata":{"id":"cxpUNe57OpiX"},"source":["### 1.IMDB 리뷰 데이터 전처리하기\r\n","\r\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2726,"status":"ok","timestamp":1610703907395,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"DIk_rJy0Opk9"},"outputs":[],"source":["from tensorflow.keras.datasets import imdb\r\n","from tensorflow.keras.utils import to_categorical\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","metadata":{"id":"hLSJXy-8OpnP"},"source":["IMDB 리뷰 데이터는 앞서 텍스트 분류하기 챕터에서 다룬 바 있으므로 데이터에 대한 상세 설명은 생략한다. 최대 단어 개수를 10,000으로 제한하고 훈련 데이터와 테스트 데이터를 받아온다.\r\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9083,"status":"ok","timestamp":1610703913760,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"qjMaF6dXOppc","outputId":"06510432-8073-42dd-b81a-9b2225b1c83f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"]},{"name":"stderr","output_type":"stream","text":["\u003cstring\u003e:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"]}],"source":["vocab_size = 10000\r\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"EsCVh10POprz"},"source":["훈련 데이터와 이에 대한 레이블이 각각 X_train, y_train에, 테스트 데이터와 이에 대한 레이블이 각각 X_test, y_test에 저장되었다. IMDB 리뷰 데이터는 이미 정수 인코딩이 된 상태이므로 남은 전처리는 패딩 뿐이다. 리뷰의 최대 길이와 평균 길이를 확인해보자.\r\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9077,"status":"ok","timestamp":1610703913760,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"QVE8VhbgOpto","outputId":"da18e8ac-66c0-469f-e88f-518ee9ad113d"},"outputs":[{"name":"stdout","output_type":"stream","text":["리뷰의 최대 길이 : 2494\n","리뷰의 평균 길이 : 238.71364\n"]}],"source":["print('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\r\n","print('리뷰의 평균 길이 : {}'.format(sum(map(len, X_train)) / len(X_train)))"]},{"cell_type":"markdown","metadata":{"id":"X4XUnIShOpv4"},"source":["리뷰의 최대 길이는 2,494이며 리뷰의 평균 길이는 약 238로 확인된다. 평균 길이보다는 조금 크게 데이터를 패딩하겠다.\r\n","\r\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10640,"status":"ok","timestamp":1610703915329,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"51hnwTzkOp0h"},"outputs":[],"source":["max_len = 500\r\n","X_train = pad_sequences(X_train, maxlen = max_len)\r\n","X_test  = pad_sequences(X_test , maxlen = max_len)"]},{"cell_type":"markdown","metadata":{"id":"CwATBv_iOp2o"},"source":["훈련용 리뷰와 테스트용 리뷰의 길이가 둘 다 500이 되었다.\r\n"]},{"cell_type":"markdown","metadata":{"id":"syi09s7gOp9X"},"source":["### 2.바다나우 어텐션(Bahdanau Attention)\r\n","\r\n","여기서 사용할 어텐션은 바다나우 어텐션(Bahdanau Attention)이다. 이를 이해하기 위해 앞서 배운 가장 쉬운 어텐션이었던 닷 프로덕트 어텐션과 어텐션 스코어 함수의 정의를 상기해보자.\r\n","\r\n","어텐션 스코어 함수란 주어진 query와 모든 key에 대해서 유사도를 측정하는 함수를 말한다. 그리고 닷 프로덕트 어텐션에서는 query와 key의 유사도를 구하는 방법이 내적(dot product)이었다. 다음은 닷 프로덕트 어텐션의 어텐션 스코어 함수를 보여준다.\r\n","\r\n","$score(query,\\ key) = query^Tkey$\r\n","\r\n","바다나우 어텐션은 아래와 같은 어텐션 스코어 함수를 사용한다.\r\n","\r\n","$score(query,\\ key) = V^Ttanh(W_{1}key + W_{2}query)$\r\n","\r\n","이 어텐션 스코어 함수를 사용하여 어텐션 메커니즘을 구현하면 된다. 그런데 텍스트 분류에서 어텐션 메커니즘을 사용하는 이유는 무엇일까? RNN의 마지막 은닉 상태는 예측을 위해 사용된다. 그런데 이 RNN의 마지막 은닉 상태는 몇 가지 유용한 정보들을 손실한 상태이다. 그래서 RNN이 time step을 지나며 손실했던 정보들을 다시 참고하고자 한다.\r\n","\r\n","이는 다시 말해 RNN의 모든 은닉 상태들을 다시 한 번 참고하겠다는 것이다. 그리고 이를 위해서 어텐션 메커니즘을 사용한다.\r\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10638,"status":"ok","timestamp":1610703915331,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"Wd07FukxOp_3"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10634,"status":"ok","timestamp":1610703915331,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"RrspTbz5RkEq"},"outputs":[],"source":["class BahdanauAttention(tf.keras.Model):\r\n","\r\n","    def __init__(self, units):\r\n","        super(BahdanauAttention, self).__init__()\r\n","        self.W1 = Dense(units)\r\n","        self.W2 = Dense(units)\r\n","        self.V = Dense(1)\r\n","\r\n","    def call(self, values, query): # 단, key와 value는 같음\r\n","        # query shape == (batch_size, hidden_size)\r\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\r\n","        # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해준다.\r\n","        hidden_with_time_axis = tf.expand_dims(query, 1)\r\n","\r\n","        # score shape == (batch_size, max_length, 1)\r\n","        # we get 1 at the last axis because we are applying score to self.V\r\n","        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\r\n","        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\r\n","\r\n","        # attention_weights shape == (batch_size, max_length, 1)\r\n","        attention_weights = tf.nn.softmax(score, axis = 1)\r\n","\r\n","        # context_vector shape after sum == (batch_size, hidden_size)\r\n","        context_vector = attention_weights * values\r\n","        context_vector = tf.reduce_sum(context_vector, axis = 1)\r\n","\r\n","        return context_vector, attention_weights\r\n","        "]},{"cell_type":"markdown","metadata":{"id":"nFWzC0mrUBiX"},"source":["### 3.양방향 LSTM + 어텐션 메커니즘(BiLSTM with Attention Mechanism)\r\n","\r\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10630,"status":"ok","timestamp":1610703915332,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"n7zc2YL9UJsj"},"outputs":[],"source":["from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\r\n","from tensorflow.keras import Input, Model\r\n","from tensorflow.keras import optimizers\r\n","import os"]},{"cell_type":"markdown","metadata":{"id":"5A4-Y7tTUUza"},"source":["이제 모델을 설계해보겠다. 여기서는 케라스의 함수형 API를 사용한다. 우선 입력층과 임베딩층을 설계한다.\r\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":16390,"status":"ok","timestamp":1610703921097,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"W-M1IlWnWaMU"},"outputs":[],"source":["sequence_input = Input(shape = (max_len, ), dtype = 'int32')\r\n","embedded_sequences = Embedding(vocab_size, 128, input_length = max_len, mask_zero = True)(sequence_input)\r\n"]},{"cell_type":"markdown","metadata":{"id":"xYH9Fae-WaO6"},"source":["10,000개의 단어들을 128차원의 벡터로 임베딩하도록 설계하였다. 이제 양방향 LSTM을 설계한다. 단, 여기서는 양방향 LSTM을 두 층을 사용하겠다. 우선, 첫번째 층이다. 두번째 층을 위에 쌓을 예정이므로 return_sequences를 True로 해주어야 한다.\r\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":18523,"status":"ok","timestamp":1610703923234,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"xcs6nBIMWaQ2"},"outputs":[],"source":["lstm = Bidirectional(LSTM(64, dropout = 0.5, return_sequences = True))(embedded_sequences)\r\n"]},{"cell_type":"markdown","metadata":{"id":"bL4IqFZGWaTD"},"source":["두번째 층을 설계한다. 상태를 리턴받아야 하므로 return_state를 True로 해주어야 한다. "]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":20375,"status":"ok","timestamp":1610703925092,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"av7rNpMVWaVd"},"outputs":[],"source":["lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(64, dropout = 0.5,\r\n","                                                                        return_sequences = True,\r\n","                                                                        return_state = True))(lstm)\r\n","                                                                        "]},{"cell_type":"markdown","metadata":{"id":"WNb7_PlYWaZq"},"source":["각 상태의 크기(shape)를 출력해보겠다.\r\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20371,"status":"ok","timestamp":1610703925094,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"r8MsVdPxWab4","outputId":"560c75b2-acc5-4c24-c3ec-4461e3a9babb"},"outputs":[{"name":"stdout","output_type":"stream","text":["(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n"]}],"source":["print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)"]},{"cell_type":"markdown","metadata":{"id":"RlP3fLomWael"},"source":["순방향 LSTM의 은닉 상태와 셀 상태를 forward_h, forward_c에 저장하고, 역방향 LSTM의 은닉 상태와 셀 상태를 backward_h, backward_c에 저장한다.\r\n","\r\n","각 은닉 상태나 셀 상태의 경우에는 128차원을 가지는데, lstm의 경우에는 (500 x 128)의 크기를 가진다. forward 방향과 backward 방향이 연결된 hidden state 벡터가 모든 시점에 대해서 존재함을 의미한다.\r\n","\r\n","양방향 LSTM을 사용할 경우에는 순방향 LSTM과 역방향 LSTM 각각 은닉 상태와 셀 상태를 가지므로, 양방향 LSTM의 은닉 상태와 셀 상태를 사용하려면 두 방향의 LSTM의 상태들을 연결(concatenate)해주면 된다.\r\n","\r\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":20365,"status":"ok","timestamp":1610703925094,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"ZTGjdeAJWagh"},"outputs":[],"source":["state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\r\n","state_c = Concatenate()([forward_c, backward_c]) # 셀 상태"]},{"cell_type":"markdown","metadata":{"id":"-RYdgta2Waiz"},"source":["어텐션 메커니즘에서는 은닉 상태를 사용한다. 이를 입력으로 컨텍스트 벡터(context vector)를 얻는다.\r\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":20361,"status":"ok","timestamp":1610703925095,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"JhhKVkH3WalQ"},"outputs":[],"source":["attention = BahdanauAttention(64) # 가중치 크기 정의\r\n","context_vector, attention_weights = attention(lstm, state_h)"]},{"cell_type":"markdown","metadata":{"id":"DNPoFAT4WanR"},"source":["컨텍스트 벡터를 밀집층(dense layer)에 통과시키고, 이진 분류이므로 최종 출력층에 1개의 뉴런을 배치하고, 활성화 함수로 시그모이드 함수를 사용한다.\r\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":20357,"status":"ok","timestamp":1610703925095,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"XDlFPJOFaaS7"},"outputs":[],"source":["dense1 = Dense(20, activation = 'relu')(context_vector)\r\n","dropout = Dropout(0.5)(dense1)\r\n","output = Dense(1, activation = 'sigmoid')(dropout)\r\n","model = Model(inputs = sequence_input,\r\n","              outputs = output)"]},{"cell_type":"markdown","metadata":{"id":"dngREVWYaaVd"},"source":["옵티마이저로 아담 옵티마이저를 사용하고, 모델을 컴파일한다.\r\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":20352,"status":"ok","timestamp":1610703925095,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"u3x6g0N5aaXr"},"outputs":[],"source":["model.compile(loss = 'binary_crossentropy',\r\n","              optimizer = 'adam',\r\n","              metrics = ['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"9vZOfAOFaaaC"},"source":["시그모이드 함수를 사용하므로 손실 함수로 binary_crossentropy를 사용하였다. 이제 모델을 훈련하겠다.\r\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1952179,"status":"ok","timestamp":1610705856928,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"SURBcvUBaacP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","98/98 [==============================] - 1331s 13s/step - loss: 0.5941 - accuracy: 0.6471 - val_loss: 0.2905 - val_accuracy: 0.8797\n","Epoch 2/3\n","98/98 [==============================] - 1270s 13s/step - loss: 0.2438 - accuracy: 0.9135 - val_loss: 0.2991 - val_accuracy: 0.8798\n","Epoch 3/3\n","98/98 [==============================] - 1323s 14s/step - loss: 0.1849 - accuracy: 0.9400 - val_loss: 0.3190 - val_accuracy: 0.8751\n"]}],"source":["hist = model.fit(X_train, y_train,\r\n","                 epochs = 3, batch_size = 256,\r\n","                 validation_data = (X_test, y_test),\r\n","                 verbose = 1)"]},{"cell_type":"markdown","metadata":{"id":"vz7gbBUCaagh"},"source":["검증 데이터로 테스트 데이터를 사용하여 에포크가 끝날 때마다 테스트 데이터에 대한 정확도를 출력하도록 하였다. "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2436451,"status":"ok","timestamp":1610706341202,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"id":"skWKv57IWapw","outputId":"a8df4b84-cf33-4da3-e303-350b63ebd9af"},"outputs":[{"name":"stdout","output_type":"stream","text":["782/782 [==============================] - 484s 619ms/step - loss: 0.3109 - accuracy: 0.8784\n","\n"," 테스트 정확도: 0.8784\n"]}],"source":["print('\\n 테스트 정확도: %.4f' % (model.evaluate(X_test, y_test)[1]))\r\n"]},{"cell_type":"markdown","metadata":{"id":"dABfbRvSbSkz"},"source":["87.93%의 정확도를 얻는다. 위 모델을 네이버 영화 리뷰 분류하기에도 수행해보아라. 저자의 경우 네이버 영화 리뷰 분류하기에 위 모델을 그대로 적용하였을 때, 86%의 정확도를 얻었다.\r\n"]},{"cell_type":"markdown","metadata":{"id":"qwjz3P3abcR4"},"source":["어텐션 메커니즘 참고 자료 :\r\n","\r\n","NTM with Attention : https://www.tensorflow.org/tutorials/text/nmt_with_attention\r\n","\r\n","Text classification using BiLSTM with attention :\r\n","https://androidkt.com/text-classification-using-attention-mechanism-in-keras/\r\n","\r\n","https://matthewmcateer.me/blog/getting-started-with-attention-for-classification/\r\n","\r\n","https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/\r\n","\r\n","그래디언트 클리핑 참고 자료 : 머신 러닝 교과서 with 파이썬, 사이킷런, 텐서플로\r\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOIPoBHKDRyX1KPZuCtNjkE","collapsed_sections":[],"name":"16. 어텐션 메커니즘 / 4) 양방향 LSTM과 어텐션 메커니즘.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}