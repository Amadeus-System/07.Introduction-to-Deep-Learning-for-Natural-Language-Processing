{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03. 언어 모델 / 5) 펄플렉서티.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMM1S0vaebMPS3K9JZ9Oeqc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IgKRODhK2tU7"},"source":["## 5) 펄플렉서티(Perplexity)\n","\n","두 개의 모델 A, B가 있을 때 이 모델의 성능은 어떻게 비교할 수 있을까? 두 개의 모델을 오타 교정, 기계 번역 등의 평가에 투입해볼 수 있겠다. 그리고 두 모델이 해당 업무의 성능을 누가 더 잘했는지를 비교하면 되겠다. 그런데 두 모델의 성능을 비교하고자, 일일이 모델들에 대해서 실제 작업을 시켜보고 정확도를 비교하는 작업은 공수가 너무 많이 드는 작업이다. 만약 비교해야하는 모델이 두 개가 아니라 그 이상의 수라면 시간은 비교해야하는 모델의 수만큼 배로 늘어날 수도 있다.\n","\n","이러한 평가를 외부 평가(extrinsic evaluation)라고 하는데, 이러한 평가보다는 어쩌면 조금은 부정확할 수는 있어도 테스트 데이터에 대해서 빠르게 식으로 계산되는 더 간단한 평가 방법이 있다. 바로 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 내부 평가(Intrinsic evaluation)에 해당되는 펄플렉서티(perplexity)이다."]},{"cell_type":"markdown","metadata":{"id":"f-arnXNg213P"},"source":["### 1. 언어 모델의 평가 방법(Evaluation metric) : PPL\n","\n","펄플렉서티(perplexity)는 언어 모델을 평가하기 위한 내부 평가 지표이다. 보통 줄여서 PPL이라고 표현한다. 왜 perplexity라는 용어를 사용했을까? 영어에서 'perplexed'는 '헷갈리는'과 유사한 의미를 가진다. 그러니까 여기서 PPL은 '헷갈리는 정도'로 이해하자. PPL를 처음 배울때 다소 낯설게 느껴질 수 있는 점이 있다면, PPL은 수치가 높으면 좋은 성능을 의미하는 것이 아니라, '낮을수록' 언어 모델의 성능이 좋다는 것을 의미한다는 점이다.\n","\n","PPL은 단어의 수로 정규화(normalization) 된 테스트 데이터에 대한 확률의 역수이다. PPL을 최소화한다는 것은 문장의 확률을 최대화하는 것과 같다. 문장 $W$의 길이가 $N$이라고 하였을 때의 PPL은 다음과 같다.\n","\n","$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}$\n","\n","문장의 확률에 체인룰(chain rule)을 적용하면 아래와 같다.\n","\n","$PPL(W)=\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{1}, w_{2}, ... , w_{i-1})}}$\n","\n","여기에 n-gram을 적용해볼 수도 있다. 예를 들어 bigram 언어 모델의 경우에는 식이 아래와 같다.\n","\n","$PPL(W)=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{i-1})}}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tD5IdsRq215Z"},"source":["### 2. 분기 계수(Branching factor)\n","\n","PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)이다. PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미한다. 가령, 언어 모델에 어떤 테스트 데이터를 주고 측정했더니 PPL이 10이 나왔다고 해보자. 그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time-step)마다 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있다. 같은 테스트 데이터에 대해서 두 언어 모델의 PPL을 각각 계산 후에 PPL의 값을 비교하면, 두 언어 모델 중 어떤 것이 성능이 좋은지도 판단이 가능하다. 당연히 PPL이 더 낮은 언어 모델의 성능이 더 좋다고 볼 수 있다.\n","\n","$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\frac{1}{N}}=(\\frac{1}{10}^{N})^{-\\frac{1}{N}}=\\frac{1}{10}^{-1}=10$\n","\n","단, 평가 방법에 있어서 주의할 점은 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보인다는 것이지, 사람이 직접 느끼기에 좋은 언어 모델이라는 것을 반드시 의미하진 않는다는 점이다. 또한 언어 모델의 PPL은 테스트 데이터에 의존하므로 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많고, 또한 도메인에 알맞은 동일한 테스트 데이터를 사용해야 신뢰도가 높다는 것이다."]},{"cell_type":"markdown","metadata":{"id":"4umpJb1F217e"},"source":["### 3. 기존 언어 모델 VS 인공 신경망을 이용한 언어 모델\n","\n","PPL의 실제 사용 사례를 확인해보겠다. 페이스북 AI 연구팀은 우리가 앞서 배운 n-gram 언어 모델과 이후 배우게 될 딥러닝을 이용한 언어 모델에 대해서 PPL로 성능 테스트를 한 표를 공개한 바 있다.\n","\n","<img src = 'https://wikidocs.net/images/page/21697/ppl.PNG' width = 60%>\n","\n","링크 : https://research.fb.com/building-an-efficient-neural-language-model-over-a-billion-words/\n","\n","표에서 맨 위의 줄의 언어 모델이 n-gram을 이용한 언어 모델이며 PPL이 67.6으로 측정되었다. 5-gram을 사용하였으며, 5-gram 앞에 Interpolated Kneser-Ney라는 이름이 붙었는데 이 책에서는 별도 설명을 생략하겠다고 했던 일반화(generalization) 방법이 사용된 모델이다. 반면, 그 아래의 모델들은 인공 신경망을 이용한 언어 모델들로 페이스북 AI 연구팀이 자신들의 언어 모델을 다른 언어 모델과 비교하고자 하는 목적으로 기록하였다. 아직 RNN과 LSTM 등이 무엇인지 배우지는 않았지만, 인공 신경망을 이용한 언어 모델들은 대부분 n-gram을 이용한 언어 모델보다 더 좋은 성능 평가를 받았음을 확인할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"tckPz0ul219o"},"source":["추천하는 참고 링크 : \n","\n","https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3\n","\n","https://www.slideshare.net/shkulathilake/nlpkashkevaluating-language-model\n","\n","http://www.statmt.org/book/slides/07-language-models.pdf\n","\n","https://github.com/kobikun/wiki/wiki/Language-Models\n","\n"]}]}