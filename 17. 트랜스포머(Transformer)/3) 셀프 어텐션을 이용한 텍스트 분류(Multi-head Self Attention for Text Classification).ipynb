{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3) 셀프 어텐션을 이용한 텍스트 분류(Multi-head Self Attention for Text Classification).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP6YFy4HYzRNQBtYGCGB4HE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9bcT19gDMvgH"},"source":["## 3) 셀프 어텐션을 이용한 텍스트 분류(Multi-head Self Attention for Text Classification)\r\n","\r\n","트랜스포머는 RNN 계열의 seq2seq를 대체하기 위해서 등장했다. 그리고 트랜스포머의 인코더는 RNN 인코더를, 트랜스포머의 디코더는 RNN 디코더를 대체할 수 있었다.\r\n","\r\n","트랜스포머의 인코더는 셀프 어텐션이라는 메커니즘을 통해 문장을 이해한다. RNN과 그 동작 방식은 다르지만, RNN이 텍스트 분류나 개체명 인식과 같은 다양한 자연어 처리 태스크에 쓰일 수 있다면 트랜스포머의 인코더 또한 가능할 것이다.\r\n","\r\n","실제로 트랜스포머의 인코더는 다양한 분야의 자연어 처리 태스크에서 사용될 수 있었고, 이 아이디어는 후에 배우게 될 BERT라는 모델로 이어지게 된다. 이번 챕터에서는 트랜스포머의 인코더를 사용하여 텍스트 분류를 수행한다.\r\n"]},{"cell_type":"markdown","metadata":{"id":"jVYXt77_MzZo"},"source":["### 1.멀티 헤드 어텐션"]},{"cell_type":"code","metadata":{"id":"PvFMmLYmMzb1","executionInfo":{"status":"ok","timestamp":1611048020179,"user_tz":-540,"elapsed":3675,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import tensorflow as tf"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXN5mxx0MzeU","executionInfo":{"status":"ok","timestamp":1611048020742,"user_tz":-540,"elapsed":1909,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\r\n","    def __init__(self, embedding_dim, num_heads=8):\r\n","        super(MultiHeadAttention, self).__init__()\r\n","        self.embedding_dim = embedding_dim # d_model\r\n","        self.num_heads = num_heads\r\n","\r\n","        assert embedding_dim % self.num_heads == 0\r\n","\r\n","        self.projection_dim = embedding_dim // num_heads\r\n","        self.query_dense = tf.keras.layers.Dense(embedding_dim)\r\n","        self.key_dense = tf.keras.layers.Dense(embedding_dim)\r\n","        self.value_dense = tf.keras.layers.Dense(embedding_dim)\r\n","        self.dense = tf.keras.layers.Dense(embedding_dim)\r\n","\r\n","    def scaled_dot_product_attention(self, query, key, value):\r\n","        matmul_qk = tf.matmul(query, key, transpose_b=True)\r\n","        depth = tf.cast(tf.shape(key)[-1], tf.float32)\r\n","        logits = matmul_qk / tf.math.sqrt(depth)\r\n","        attention_weights = tf.nn.softmax(logits, axis=-1)\r\n","        output = tf.matmul(attention_weights, value)\r\n","        return output, attention_weights\r\n","\r\n","    def split_heads(self, x, batch_size):\r\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\r\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n","\r\n","    def call(self, inputs):\r\n","        # x.shape = [batch_size, seq_len, embedding_dim]\r\n","        batch_size = tf.shape(inputs)[0]\r\n","\r\n","        # (batch_size, seq_len, embedding_dim)\r\n","        query = self.query_dense(inputs)\r\n","        key = self.key_dense(inputs)\r\n","        value = self.value_dense(inputs)\r\n","\r\n","        # (batch_size, num_heads, seq_len, projection_dim)\r\n","        query = self.split_heads(query, batch_size)  \r\n","        key = self.split_heads(key, batch_size)\r\n","        value = self.split_heads(value, batch_size)\r\n","\r\n","        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\r\n","        # (batch_size, seq_len, num_heads, projection_dim)\r\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \r\n","\r\n","        # (batch_size, seq_len, embedding_dim)\r\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\r\n","        outputs = self.dense(concat_attention)\r\n","        return outputs"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AlneCWQ4Mzga"},"source":["### 2.인코더 설계하기\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"7oPeVhPmMzip","executionInfo":{"status":"ok","timestamp":1611048024217,"user_tz":-540,"elapsed":1117,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["class TransformerBlock(tf.keras.layers.Layer):\r\n","    def __init__(self, embedding_dim, num_heads, dff, rate=0.1):\r\n","        super(TransformerBlock, self).__init__()\r\n","        self.att = MultiHeadAttention(embedding_dim, num_heads)\r\n","        self.ffn = tf.keras.Sequential(\r\n","            [tf.keras.layers.Dense(dff, activation=\"relu\"),\r\n","             tf.keras.layers.Dense(embedding_dim),]\r\n","        )\r\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\r\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\r\n","\r\n","    def call(self, inputs, training):\r\n","        attn_output = self.att(inputs)\r\n","        attn_output = self.dropout1(attn_output, training=training)\r\n","        out1 = self.layernorm1(inputs + attn_output)\r\n","        ffn_output = self.ffn(out1)\r\n","        ffn_output = self.dropout2(ffn_output, training=training)\r\n","        return self.layernorm2(out1 + ffn_output)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXzB5YsCMzku"},"source":["### 3.포지션 임베딩\r\n"]},{"cell_type":"code","metadata":{"id":"VaPpDYB9Mzm-","executionInfo":{"status":"ok","timestamp":1611048043010,"user_tz":-540,"elapsed":865,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\r\n","    def __init__(self, max_len, vocab_size, embedding_dim):\r\n","        super(TokenAndPositionEmbedding, self).__init__()\r\n","        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n","        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\r\n","\r\n","    def call(self, x):\r\n","        max_len = tf.shape(x)[-1]\r\n","        positions = tf.range(start=0, limit=max_len, delta=1)\r\n","        positions = self.pos_emb(positions)\r\n","        x = self.token_emb(x)\r\n","        return x + positions"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJK-u4m6MzpY"},"source":["### 4.데이터 로드 및 전처리\r\n"]},{"cell_type":"code","metadata":{"id":"VG3cgeq5Mzrj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611048064996,"user_tz":-540,"elapsed":6521,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"312577a5-81a9-4d36-ef38-a98dd25ca51c"},"source":["vocab_size = 20000  # Only consider the top 20k words\r\n","max_len = 200  # Only consider the first 200 words of each movie review\r\n","\r\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\r\n","print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\r\n","print('테스트용 리뷰 개수 : {}'.format(len(X_test)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"],"name":"stderr"},{"output_type":"stream","text":["훈련용 리뷰 개수 : 25000\n","테스트용 리뷰 개수 : 25000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"PJNl0rteMzuJ","executionInfo":{"status":"ok","timestamp":1611048070734,"user_tz":-540,"elapsed":1917,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\r\n","X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIa6F55CMzwQ"},"source":["### 5.트랜스포머를 이용한 IMDB 리뷰 분류\r\n"]},{"cell_type":"code","metadata":{"id":"o4fPjwkeMzyu","executionInfo":{"status":"ok","timestamp":1611048105637,"user_tz":-540,"elapsed":1590,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["embedding_dim = 32  # Embedding size for each token\r\n","num_heads = 2  # Number of attention heads\r\n","dff = 32  # Hidden layer size in feed forward network inside transformer\r\n","\r\n","inputs = tf.keras.layers.Input(shape=(max_len,))\r\n","embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\r\n","x = embedding_layer(inputs)\r\n","transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\r\n","x = transformer_block(x)\r\n","x = tf.keras.layers.GlobalAveragePooling1D()(x)\r\n","x = tf.keras.layers.Dropout(0.1)(x)\r\n","x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\r\n","x = tf.keras.layers.Dropout(0.1)(x)\r\n","outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\r\n","\r\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYmxXweO8zA-","executionInfo":{"status":"ok","timestamp":1611048275286,"user_tz":-540,"elapsed":164966,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"d02c8a4d-467f-44ed-8857-bdaca4b673f5"},"source":["model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\r\n","history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))\r\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","782/782 [==============================] - 83s 104ms/step - loss: 0.5030 - accuracy: 0.7227 - val_loss: 0.3203 - val_accuracy: 0.8627\n","Epoch 2/2\n","782/782 [==============================] - 81s 103ms/step - loss: 0.1961 - accuracy: 0.9276 - val_loss: 0.3161 - val_accuracy: 0.8686\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fh8xjDK-80jK","executionInfo":{"status":"ok","timestamp":1611048295199,"user_tz":-540,"elapsed":176918,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"60d318ca-ff98-4937-de9c-2e3cf777518f"},"source":["print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["782/782 [==============================] - 20s 26ms/step - loss: 0.3161 - accuracy: 0.8686\n","테스트 정확도: 0.8686\n"],"name":"stdout"}]}]}