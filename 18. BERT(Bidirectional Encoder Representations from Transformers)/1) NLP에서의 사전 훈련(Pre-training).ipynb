{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1) NLP에서의 사전 훈련(Pre-training).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNmNRzTukbn6WXX9sYUMiTi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WnoT0W3kbuid"},"source":["# 18.BERT(Bidirectional Encoder Representations from Transformers)\n","\n","내용\n"]},{"cell_type":"markdown","metadata":{"id":"CHRVQNSTbv2Y"},"source":["## 1) NLP에서의 사전 훈련(Pre-training)\n","\n","2018년 딥러닝 연구원 세바스찬 루더는 사전 훈련된 언어 모델의 약진을 보며 다음과 같은 말을 했다.\n","\n","```\n","\"사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다.\"\n","```\n","\n","BERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 계열의 모델들이 자연어 처리를 지배했던 19년과 20년을 회고하면 이 말은 이미 현실이 되었다.\n","\n","이번 챕터는 BERT를 배우기에 앞서 자연어 처리의 트렌드를 정리해보고자 준비한 챕터이다. 앞서 배운 사전 훈련된 워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리가 어떤 방향으로 발전되어 왔는지 정리해보자.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"COurheF9bv4d"},"source":["### 1.사전 훈련된 워드 임베딩\n","\n","우리는 앞서 Word2Vec, FastText, GloVe와 같은 워드 임베딩 방법론들을 학습했다. 어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 있다. 임베딩 층(Embedding layer)을 랜덤 초기화하여 처음부터 학습하는 방법과 방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 이미 학습된 임베딩 벡터들을 가져와 사용하는 방법이다. 만약, 태스크에 사용하기 위한 데이터가 적다면, 사전 훈련된 임베딩을 사용하면 성능을 높일 수 있었고, 자연어 처리에서 광범위하게 사용되어져 왔다.\n","\n","그런데 랜덤 초기화 학습을 하든, Word2Vec을 사용하든, 이러한 임베딩에는 한 가지 문제점이 존재해왔는데 하나의 단어가 하나의 벡터값으로 맵핑되므로 문맥을 고려하지 못한다는 점이었다. 단적으로, 다의어나 동음이의어를 구분하지 못하는 문제점이 있었다. 한국어에는 '사과'라는 단어가 존재하는데 이 '사과'는 용서를 빈다는 의미로도 쓰이지만, 먹는 과일의 의미로도 사용된다. 그러나 임베딩 벡터는 '사과'라는 벡터에 하나의 벡터값을 맵핑하므로 이 두 가지 의미를 구분할 수 없었다.\n","\n","이 한계는 사전 훈련된 언어 모델을 사용함으로써 극복할 수 있었는데, 이는 아래 ELMo에서 다시 재언급하겠다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Oktos-AObv6p"},"source":["### 2.사전 훈련된 언어 모델\n","\n","<img src = 'https://wikidocs.net/images/page/108730/image1.PNG'>\n","\n","2015년 구글은 'Semi-supervised Sequence Learning'이라는 논문에서 LSTM 언어 모델을 학습하고나서 이렇게 학습한 LSTM을 텍스트 분류에 추가 학습하는 방법을 제안했다. 이 방법은 우선 LSTM 언어 모델을 학습한다. 언어 모델은 주어진 텍스트로부터 이전 단어들로부터 다음 단어를 예측하도록 학습하므로 기본적으로 별도의 레이블이 부착되지 않은 텍스트 데이터로도 학습 가능하다. 그리고 이렇게 레이블이 없는 데이터로 학습된 LSTM과 가중치가 랜덤으로 초기화된 LSTM 두 가지를 두고, IMDB 리뷰 데이터를 학습하여 정확도를 테스트한다면 전자의 LSTM이 좀 더 좋은 성능을 얻는다는 것이다.\n","\n","방대한 텍스트로 LSTM 언어 모델을 학습해두고, 이러한 언어 모델을 다른 태스크에 사용하는 방법으로는 앞서 우리가 이전 챕터에서 학습한 ELMo와 같은 아이디어도 존재한다.\n","\n","<img src = 'https://wikidocs.net/images/page/108730/image2.PNG'>\n","\n","ELMo는 순방향 언어 모델과 역방향 언어 모델을 각각 따로 학습시킨 후에, 이렇게 사전 학습된 언어 모델로부터 임베딩 값을 얻는다는 아이디어였다. 이 임베딩은 문장에 따라서 '사과'와 같이 의미는 다르지만 소리가 같은 단어들이라고 하더라도 임베딩 벡터값이 달라지므로, Word2Vec이나 GloVe 등과 같은 워드 임베딩의 문제점을 해결할 수 있었다.\n","\n","트랜스포머가 등장하고 나서, 이제 언어 모델을 학습할 때 LSTM이 아닌 트랜스포머를 사용하는 시도가 등장하기 시작했다.\n","\n","<img src = 'https://wikidocs.net/images/page/108730/image3.PNG'>\n","\n","위의 그림에서 Trm은 트랜스포머(Transformer)의 약자이다. 트랜스포머의 디코더는 LSTM 언어 모델처럼 순차적으로 이전 단어들로부터 다음 단어를 예측한다. Open AI는 트랜스포머 디코더로 총 12개의 층을 쌓은 후, 방대한 텍스트 데이터로 언어 모델 GPT-1을 만들었다. Open AI는 GPT-1에 여러 다양한 태스크를 위해 추가 학습을 시켰을 때, 높은 성능을 얻을 수 있음을 보여주었다.\n","\n","이제 NLP의 트렌드는 사전 훈련된 언어 모델을 만들어, 이를 다른 태스크에 추가 학습하여 높은 성능을 얻는 것으로 넘어오게 되었다. 그리고 이를 더 잘하기 위해서 기존의 언어 모델의 구조를 바꿔보고자 하는 시도가 생겨났다.\n","\n","<img src = 'https://wikidocs.net/images/page/108730/image4.PNG'>\n","\n","위의 좌측 그림에 있는 단방향 언어모델은 우리가 지금까지 배운 전형적인 언어모델이다. 시작 토큰 <(SOS)>가 들어가면, 다음 단어 I를 예측하고 그리고 그 다음 단어 am을 예측한다. 반면, 우측에 있는 양방향 언어 모델은 우리가 본 적 없던 형태의 언어 모델이다. 실제로 이렇게 구현하는 경우는 거의 없는데 그 이유는 무엇일까? 가령, 양방향 LSTM을 이용해서 우측과 같은 언어 모델을 만들었다고 해보자. 초록색 LSTM 셀은 순방향 언어 모델로 <(sos)>를 입력받아 I를 예측하고, 그 후에 am을 예측한다. 그런데 am을 예측할 때, 출력층은 주황색 LSTM 셀인 역방향 언어 모델의 정보도 함께 받고있다. 그런데 am을 예측하는 시점에서 역방향 언어 모델이 이미 관측한 단어는 a, am, I 이렇게 3개의 단어이다. 이미 예측해야하는 단어를 역방향 언어 모델을 통해 관측한 셈이므로 언어 모델은 일반적으로 이렇게 양방향으로 구현하지는 않는다.\n","\n","하지만 언어의 문맥이라는 것은 실제로 양방향으로 존재한다. 텍스트 분류나 개체명 인식 등에서 양방향 LSTM을 사용하여 모델을 구현해서 좋은 성능을 얻을 수 있었던 것은 그 때문이다. 하지만 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로 인해 양방향을 사용할 수 없으므로, 그 대안으로 ELMo에서는 단방향과 역방향으로 언어 모델을 분리해서 학습하는 방법을 사용했다.\n","\n","2018년에는 언어 모델의 양방향 학습을 위해 새로운 구조의 언어 모델이 탄생했는데, 마스크드 언어 모델이다.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"myDfmtyabv80"},"source":["### 3.마스크드 언어 모델(Masked Language Model)\n","\n","마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)한다. 여기서 마스킹이란 원래의 단어가 무엇이었는지 모르게 한다는 뜻이다. 그리고 인공 신경망에게 이렇게 마스킹된 단어들(Masked words)을 예측하도록 한다. 문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식이다. 우리가 영어 시험을 볼 때 종종 마주하는 빈칸 채우기 문제에 비유할 수 있다. 예를 들어 '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'를 주고 [MASK]에 들어갈 단어를 맞추게 한다.\n","\n","이 언어 모델에 대해서는 이번 챕터의 주제인 BERT를 학습하며 더 자세히 알아보겠다.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vezZ-zR4bv-x"},"source":["https://github.com/chao-ji/semi-supervised-sequence-learning\n","\n","https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6"]}]}