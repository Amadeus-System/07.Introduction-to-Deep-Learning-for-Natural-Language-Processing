{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2) 양방향 LSTM을 이용한 품사 태깅(Part-of-speech Tagging using Bi-LSTM).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNIvxI7c/E+du/iFJfUrTA8"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KT2rK8qUeJnS"},"source":["## 2) 양방향 LSTM을 이용한 품사 태깅(Part-of-speech Tagging using Bi-LSTM)\r\n","\r\n","품사 태깅에 대해서는 이미 2챕터의 토큰화 챕터에서 배운 바 있다. 그 당시에는 NLTK와 KoNLPy를 이용해서 이미 기존에 있는 모델로 품사 태깅을 수행하였지만, 여기서는 직접 양방향 LSTM을 이용한 품사 태깅을 수행하는 모델을 만들어보도록 한다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"-ZTqsckGeKv4"},"source":["### 1.품사 태깅 데이터에 대한 이해와 전처리\r\n","\r\n","이번에는 양방향 LSTM을 사용해서 품사 태깅을 하는 모델을 만들어보겠다.\r\n"]},{"cell_type":"code","metadata":{"id":"cMrAisS4eKyT"},"source":["import nltk\r\n","import numpy as np\r\n","%matplotlib inline\r\n","import matplotlib.pyplot as plt\r\n","plt.style.use(['seaborn-white'])\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","from tensorflow.keras.utils import to_categorical\r\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0mFKn07eK0i"},"source":["NLTK를 이용하면 영어 코퍼스에 토큰화와 품사 태깅 전처리를 진행한 문장 데이터를 받아올 수 있다. 여기서는 해당 데이터를 훈련시켜 품사 태깅을 수행하는 모델을 만들어보겠다. 우선 전체 문장 샘플의 개수를 확인한다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-YjUNokeK2q","executionInfo":{"elapsed":4752,"status":"ok","timestamp":1611210173992,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"9bab524a-fd3b-49c0-9ac6-daa5104085c0"},"source":["nltk.download('treebank')\r\n","\r\n","tagged_sentences = nltk.corpus.treebank.tagged_sents() # 토큰화에 품사 태깅이 된 데이터 받아오기\r\n","print('품사 태깅이 된 문장 개수 :', len(tagged_sentences)) # 문장 샘플의 개수 출력"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","품사 태깅이 된 문장 개수 : 3914\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T5FPn5MmeK47"},"source":["이 중 첫번째 샘플만 출력해보겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPPeNXAJeK6_","executionInfo":{"elapsed":4746,"status":"ok","timestamp":1611210173993,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"9ac45d3a-a2ba-4d02-d123-1d2012bf3249"},"source":["print(tagged_sentences[0]) # 첫번째 샘플 출력"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LsWrd1zoeK9L"},"source":["품사 태깅 전처리가 수행된 첫번째 문장이 출력된 것을 볼 수 있다. 이러한 문장 샘플이 총 3,914개가 있다. 그런데 훈련을 시키려면 훈련 데이터에서 단어에 해당되는 부분과 품사 태깅 정보에 해당되는 부분을 분리시켜야 한다. 즉, [('Pierre', 'NNP'), ('Vinken', 'NNP')]와 같은 문장 샘플이 있다면 Pierre과 Vinken을 같이 저장하고, NNP와 NNP를 같이 저장할 필요가 있다.\r\n","\r\n","이런 경우 파이썬 함수 중에서 zip() 함수가 유용한 역할을 한다. zip() 함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할을 한다. (2챕터의 데이터의 분리 챕터 참고)\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"FqRgGfgBeK_Z"},"source":["sentences, pos_tags = [], []\r\n","\r\n","for tagged_sentence in tagged_sentences: # 3,914개의 문장 샘플을 1개씩 불러온다.\r\n","\r\n","    sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에, 품사 태깅 정보들은 tag_info에 저장한다.\r\n","\r\n","    sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\r\n","    pos_tags.append(list(tag_info)) # 각 샘플에서 품사 태깅 정보만 저장한다.\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tLsBWgSeLBG"},"source":["각 문장 샘플에 대해서 단어는 sentences에, 태깅 정보는 pos_tags에 저장하였다. 임의로 첫번째 문장 샘플을 출력해보겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22Sug5ULgt0S","executionInfo":{"elapsed":6123,"status":"ok","timestamp":1611210175381,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"b8e8d458-727c-46cc-e4b8-042be9d706a9"},"source":["print(sentences[0])\r\n","print(pos_tags[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n","['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Xe3H38Fgt23"},"source":["첫번째 샘플에 대해서 단어에 대해서만 sentences[0]에, 또한 품사에 대해서만 pos_tags[0]에 저장된 것을 볼 수 있다. 뒤에서 보겠지만, sentences는 예측을 위한 X에 해당되며, pos_tags는 예측 대상인 y에 해당된다. 다른 샘플들에 대해서도 처리가 되었는지 확인하기 위해 임의로 아홉번째 샘플에 대해서도 확인해보겠다.\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fM_GR4HTgt7-","executionInfo":{"elapsed":6118,"status":"ok","timestamp":1611210175382,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"e38b7ec4-b49a-4e30-8fc8-aa2935e6014c"},"source":["print(sentences[8])\r\n","print(pos_tags[8])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['We', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.']\n","['PRP', 'VBP', 'VBG', 'IN', 'NNS', 'IN', 'IN', 'NN', 'VBD', 'IN', 'NN', 'VBG', 'DT', 'JJ', 'NNS', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kZ8itbnUguAw"},"source":["단어에 대해서만 sentences[8]에, 또한 품사에 대해서만 pos_tags[8]에 저장된 것을 확인할 수 있다. 또한 첫번째 샘플과 길이가 다른 것을 볼 수 있다. 사실 3,914개의 문장 샘플의 길이는 전부 제각각이다. 전체 데이터의 길이 분포를 확인해보자.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"YWetqh_qguDF","executionInfo":{"elapsed":6110,"status":"ok","timestamp":1611210175382,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"f1dc2169-6924-4bd3-be04-08d229bb7ad9"},"source":["print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\r\n","print('샘플의 평균 길이 : %f' % (sum(map(len, sentences)) / len(sentences)))\r\n","\r\n","plt.hist([len(s) for s in sentences], bins = 50)\r\n","plt.xlabel('length of samples')\r\n","plt.ylabel('number of samples')\r\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["샘플의 최대 길이 : 271\n","샘플의 평균 길이 : 25.722024\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"6c9yzVRhguFV"},"source":["위의 그래프는 대부분의 샘플의 길이가 150 이내이며, 대부분 0~50의 길이를 가지는 것을 보여준다. 이제 케라스 토크나이저를 통해서 정수 인코딩을 진행한다. 우선 케라스 토크나이저를 다음과 같이 함수로 구현한다.\r\n"]},{"cell_type":"code","metadata":{"id":"UGhsQfBcguHp"},"source":["def tokenize(samples):\r\n","    tokenizer = Tokenizer()\r\n","    tokenizer.fit_on_texts(samples)\r\n","    return tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9uR7QfVxguJt"},"source":["문장 데이터에 대해서는 src_tokenizer를, 레이블에 해당되는 품사 태깅 정보에 대해서는 tar_tokenizer를 사용한다.\r\n"]},{"cell_type":"code","metadata":{"id":"SQaPhKGjguMH"},"source":["src_tokenizer = tokenize(sentences)\r\n","tar_tokenizer = tokenize(pos_tags)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_pN8BpbguQe"},"source":["단어 집합과 품사 태깅 정보 집합의 크기를 확인해보겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmeFNdu-jz4x","executionInfo":{"elapsed":6094,"status":"ok","timestamp":1611210175384,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"8afa9ae0-33f4-4816-cd99-05261f852baa"},"source":["vocab_size = len(src_tokenizer.word_index) + 1\r\n","tag_size = len(tar_tokenizer.word_index) + 1\r\n","print('단어 집합의 크기 : {}'.format(vocab_size))\r\n","print('태깅 정보 집합의 크기 : {}'.format(tag_size))\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["단어 집합의 크기 : 11388\n","태깅 정보 집합의 크기 : 47\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X-WmOKJtj0Tf"},"source":["이제 정수 인코딩을 수행한다.\r\n"]},{"cell_type":"code","metadata":{"id":"DR7Ik5KZj0V8"},"source":["X_train = src_tokenizer.texts_to_sequences(sentences)\r\n","y_train = tar_tokenizer.texts_to_sequences(pos_tags)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7m9lHF-j0YT"},"source":["이제 문장 데이터에 대해서 정수 인코딩이 수행된 결과는 X_train, 품사 태깅 데이터에 대해서 정수 인코딩이 수행된 결과는 y_train에 저장되었다. 정수 인코딩이 되었는지 확인을 위해 임의로 세번째 데이터를 출력해보겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmKo23zTj0bV","executionInfo":{"elapsed":6083,"status":"ok","timestamp":1611210175385,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"ce5bf14a-6f65-4449-a969-7b5be3f0deb8"},"source":["print(X_train[:2])\r\n","print(y_train[:2])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[5601, 3746, 1, 2024, 86, 331, 1, 46, 2405, 2, 131, 27, 6, 2025, 332, 459, 2026, 3], [31, 3746, 20, 177, 4, 5602, 2915, 1, 2, 2916, 637, 147, 3]]\n","[[3, 3, 8, 10, 6, 7, 8, 21, 13, 4, 1, 2, 4, 7, 1, 3, 10, 9], [3, 3, 17, 1, 2, 3, 3, 8, 4, 3, 19, 1, 9]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jSRGCJUaj0dp"},"source":["앞서 본 그래프에 따르면, 대부분의 샘플은 길이가 150 이내이다. X에 해당되는 데이터 X_train의 샘플들과 y에 해당되는 데이터 y_train 샘플들의 모든 길이를 임의로 150 정도로 맞추어 보겠다. 이를 위해서 케라스의 pad_sequences()를 사용한다.\r\n"]},{"cell_type":"code","metadata":{"id":"EakiiG03j0gR"},"source":["max_len = 150\r\n","X_train = pad_sequences(X_train, maxlen = max_len, padding = 'post')\r\n","# X_train의 모든 샘플의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움.\r\n","y_train = pad_sequences(y_train, padding = 'post', maxlen = max_len)\r\n","# y_train의 모든 샘플의 길이를 맞출 때 뒤의 공간에 숫자 0으로 채움."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2V3pA9ROj0oH"},"source":["모든 샘플의 길이가 150이 되었다. 이제 훈련 데이터와 테스트 데이터를 8:2의 비율로 분리한다.\r\n"]},{"cell_type":"code","metadata":{"id":"xHkze1H6j0qZ"},"source":["X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2,\r\n","                                                    random_state = 777)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DWD9wsuj0sp"},"source":["레이블에 해당하는 태깅 정보에 대해서 원-핫 인코딩을 수행한다.\r\n"]},{"cell_type":"code","metadata":{"id":"Lr--TUODj0vQ"},"source":["y_train = to_categorical(y_train, num_classes = tag_size)\r\n","y_test  = to_categorical(y_test , num_classes = tag_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IYW7BIcPj0xR"},"source":["이제 각 데이터에 대한 크기를 확인해보겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xGU_EJvj017","executionInfo":{"elapsed":6470,"status":"ok","timestamp":1611210175792,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"196902d2-55f9-44fe-d6fb-d66a71b23cb0"},"source":["print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\r\n","print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\r\n","\r\n","print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\r\n","print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["훈련 샘플 문장의 크기 : (3131, 150)\n","훈련 샘플 레이블의 크기 : (3131, 150, 47)\n","테스트 샘플 문장의 크기 : (783, 150)\n","테스트 샘플 레이블의 크기 : (783, 150, 47)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g6PqyAAEj04X"},"source":["### 2.양방향 LSTM(Bi-directional LSTM)으로 POS Tagger 만들기\r\n"]},{"cell_type":"code","metadata":{"id":"19gcO3uElpoF"},"source":["# from tensorflow.keras.models import Sequential\r\n","# from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\r\n","# from tensorflow.keras.optimizers import Adam\r\n","from keras.models import Sequential\r\n","from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\r\n","from keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M70y6I7llpqp","executionInfo":{"elapsed":13565,"status":"ok","timestamp":1611210182898,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"0aa33dfb-5e32-4eb1-f61c-194e0a5c9a59"},"source":["model = Sequential()\r\n","model.add(Embedding(vocab_size, 128, input_length = max_len, mask_zero = True))\r\n","model.add(Bidirectional(LSTM(256, return_sequences = True)))\r\n","model.add(TimeDistributed(Dense(tag_size, activation = 'softmax')))\r\n","\r\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 150, 128)          1457664   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 150, 512)          788480    \n","_________________________________________________________________\n","time_distributed (TimeDistri (None, 150, 47)           24111     \n","=================================================================\n","Total params: 2,270,255\n","Trainable params: 2,270,255\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0c87XPYVlpso"},"source":["model.compile(loss = 'categorical_crossentropy', \r\n","              optimizer = Adam(0.001), \r\n","              metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"JoC5yVhNlpw8","executionInfo":{"elapsed":34558,"status":"ok","timestamp":1611210203901,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"62b7e954-cfbd-4426-8d0c-8a072d7ca66d"},"source":["model.fit(X_train, y_train, \r\n","          batch_size = 128,\r\n","          epochs = 6,\r\n","          validation_data = (X_test, y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/6\n","25/25 [==============================] - 86s 3s/step - loss: 0.6140 - accuracy: 0.1238 - val_loss: 0.5068 - val_accuracy: 0.2171\n","Epoch 2/6\n","25/25 [==============================] - 76s 3s/step - loss: 0.5071 - accuracy: 0.2034 - val_loss: 0.4605 - val_accuracy: 0.3599\n","Epoch 3/6\n","25/25 [==============================] - 75s 3s/step - loss: 0.4374 - accuracy: 0.4011 - val_loss: 0.3211 - val_accuracy: 0.5167\n","Epoch 4/6\n","25/25 [==============================] - 76s 3s/step - loss: 0.2858 - accuracy: 0.5660 - val_loss: 0.1895 - val_accuracy: 0.7247\n","Epoch 5/6\n","25/25 [==============================] - 75s 3s/step - loss: 0.1601 - accuracy: 0.7803 - val_loss: 0.1047 - val_accuracy: 0.8670\n","Epoch 6/6\n","25/25 [==============================] - 76s 3s/step - loss: 0.0803 - accuracy: 0.9020 - val_loss: 0.0675 - val_accuracy: 0.9017\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f22e90b95c0>"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRsY2ZxBj07Q","executionInfo":{"elapsed":1043,"status":"ok","timestamp":1611210238069,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"44d5c203-fc62-417b-a882-96e1e1fab885"},"source":["print('\\n 테스트 정확도 : %.4f' % (model.evaluate(X_test, y_test)[1]))\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["25/25 [==============================] - 0s 8ms/step - loss: 0.0662 - accuracy: 0.9053\n","\n"," 테스트 정확도 : 0.9053\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HqqWYvthnSQw"},"source":["실제로 맞추고 있는지를 특정 테스트 데이터를 주고 직접 출력해서 확인해보겠다. 우선 인덱스로부터 단어와 품사 태깅 정보를 리턴하는 index_to_word와 index_to_tag를 만들고 이를 활용하여 실제값과 예측값을 출력한다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MWZjLACcnhLg","executionInfo":{"elapsed":1169,"status":"ok","timestamp":1611210583147,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"},"user_tz":-540},"outputId":"71b915fb-024a-40ac-c08f-6396535f9215"},"source":["index_to_word = src_tokenizer.index_word\r\n","index_to_tag  = tar_tokenizer.index_word\r\n","\r\n","i = 10 # 확인하고 싶은 테스트용 샘플의 인덱스\r\n","y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\r\n","y_predicted = np.argmax(y_predicted, axis = -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\r\n","true = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\r\n","\r\n","print('{:15}|{:5}|{}'.format('단어', '실제값', '예측값'))\r\n","print(35 * '-')\r\n","\r\n","for w, t, pred in zip(X_test[i], true, y_predicted[0]):\r\n","    if w != 0: # PAD값은 제외함.\r\n","        print('{:17}: {:7} {}'.format(index_to_word[w], index_to_tag[t].upper(), index_to_tag[pred].upper()))\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["단어             |실제값  |예측값\n","-----------------------------------\n","in               : IN      IN\n","addition         : NN      NN\n",",                : ,       ,\n","buick            : NNP     NNP\n","is               : VBZ     VBZ\n","a                : DT      DT\n","relatively       : RB      RB\n","respected        : VBN     JJ\n","nameplate        : NN      NN\n","among            : IN      IN\n","american         : NNP     NNP\n","express          : NNP     NNP\n","card             : NN      NN\n","holders          : NNS     NNS\n",",                : ,       ,\n","says             : VBZ     VBZ\n","0                : -NONE-  -NONE-\n","*t*-1            : -NONE-  -NONE-\n","an               : DT      DT\n","american         : NNP     NNP\n","express          : NNP     NNP\n","spokeswoman      : NN      NN\n",".                : .       .\n"],"name":"stdout"}]}]}