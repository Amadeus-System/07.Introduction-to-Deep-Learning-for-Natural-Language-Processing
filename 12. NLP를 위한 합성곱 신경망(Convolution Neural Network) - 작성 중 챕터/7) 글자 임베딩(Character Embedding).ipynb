{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7) 글자 임베딩(Character Embedding).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNoeq/TAT2dAHwrgP7n8djB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YLM9NQChIFQQ"},"source":["## 7) 글자 임베딩(Character Embedding)\n","\n","이번 챕터에서는 워드 임베딩과는 다른 방법으로 단어의 벡터 표현 방법을 얻음으로써 OOV 문제를 해결하는 글자 임베딩(Character Embedding)의 방법에 대해서 알아보겠다. 사실 이번 챕터를 10챕터의 워드 임베딩 챕터에 넣는 것을 고려하였으나, 1D CNN의 개념을 이해하고나서 글자 임베딩의 개념을 이해할 수 있도록 12챕터에 배치하였다. \n","\n","'understand'라는 단어는 '이해하다'라는 뜻을 가진 영단어이다. 그런데 여기에 mis-를 앞에 붙여주게 되면, 'misunderstand'라는 '오해하다'라는 뜻의 다른 의미의 영단어가 된다. 비슷한 예시를 들어보겠다. 'underestimate'라는 단어는 '과소평가하다'라는 단어이다. 그렇다면 'misunderestimate'는 무슨 뜻일까? 사실 이 단어는 실제 단어가 아님에도, 우리는 이 단어의 뜻을 추측할 수 있다. 영어권 단어에서 mis-라는 접두사는 '잘못 판단하는'이라는 의미의 'mistaken'의 의미를 담고 있으므로 '과소평가하다'라는 단어 앞에 mis-라는 접두사가 붙었다면 'misunderestimate'는 '잘못 과소평가하다'라는 추측이 가능해지기 때문이다.\n","\n","글자 임베딩은 사람의 이러한 이해 능력을 흉내내는 알고리즘이다. 여기서는 각각 CNN과 RNN을 이용한 두 가지 방법을 제시한다.\n"]},{"cell_type":"markdown","metadata":{"id":"_g4LTyH7IFp6"},"source":["### 1.1D CNN을 이용한 글자 임베딩\n","\n","1D CNN은 전체 시퀀스 입력 내부의 더 작은 시퀀스로부터 정보를 얻어내는 동작을 하는 알고리즘이다. FastText가 글자의 N-gram의 조합을 이용하여 OOV 문제를 해결하였듯이, 1D CNN을 글자 임베딩에 사용할 경우에는 글자의 N-gram으로부터 정보를 얻어내게 된다. 사실 앞서 학습했던 1D CNN을 이용한 텍스트 분류를 이해했다면 글자 임베딩을 이해하는 것은 매우 간단하다. 기본적으로 단어를 글자 단위로 쪼개는 것 외에는 달라진 것이 없기 때문이다.\n","\n","<img src = 'https://wikidocs.net/images/page/116193/%EC%BA%A1%EC%B2%981.PNG'>\n","\n","위의 그림은 임의의 단어 'have'에 대해서 1D CNN을 통해서 단어 표현 벡터를 얻는 과정을 보여준다. 우선, 단어 'have'를 'h', 'a', 'v', 'e'와 같이 글자 단위로 분리한다. 그리고 임베딩 층(Embedding layer)을 이용한 임베딩을 단어에 대해서 하는 것이 아니라 글자에 대해서 하게 된다. 다시 말해 글자를 임베딩한다. (워드 임베딩 후 문장의 길이를 맞추기 위해 패딩을 했던 지난 예시들과 마찬가지로 여기서도 패딩을 할 수도 있다.) 그 후 1D CNN을 적용하게 되는데, 위의 그림은 커널의 사이즈가 4인 커널 2개, 3인 커널 2개, 2인 커널 2개를 사용할 때를 보여준다. 벡터가 6개이므로 맥스 풀링을 한 후에는 6개의 스칼라 값을 얻는데, 이렇게 얻은 스칼라값들을 전부 연결(concatenate)하여 하나의 벡터로 만들어준다.\n","\n","최종적으로 이렇게 얻은 벡터를 단어 'have'의 벡터로 사용한다. 그림에서는 글자 레벨 표현(Character-level representation)이라고 기재된 벡터에 해당된다. 이렇게 단어 벡터를 얻을 경우, 어떤 단어이든 기본적으로 글자 레벨로 쪼개므로 OOV 문제 또한 해결할 수 있다. 가령, 'docker'라는 영단어가 훈련 데이터에 없었으나 테스트 데이터에 존재하는 단어였다고 해보자. Word2Vec이나 GloVe의 경우에는 OOV 문제가 발생하게 되겠지만, 1D CNN을 이용하는 경우에는 'd', 'o', 'c', 'k', 'e', 'r'로 전부 분리되어 각 글자로 임베딩이 되고나서 1D CNN을 거친 후에 'docker'의 벡터를 얻게 되므로 OOV 문제가 발생하지 않는다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nUcl4nhtIFsY"},"source":["### 2.BiLSTM을 이용한 글자 임베딩\n","\n","글자 임베딩을 얻는 또 다른 방법으로는 BiLSTM을 이용한 방법이 있다. 1D CNN 때와 마찬가지로 기본적으로 단어를 글자로 쪼갠 후, 임베딩 층을 사용하여 글자 임베딩을 입력으로 사용하는 것은 같다. 이번 챕터도 BiLSTM을 이용한 텍스트 분류를 이해했다면 굉장히 쉽다. 11챕터의 BiLSTM을 이용한 한국어 스팀 리뷰 감성 분류하기 실습을 참고하기 바란다.\n","\n","<img src = 'https://wikidocs.net/images/page/116193/bilstm.PNG'>\n","\n","위의 그림은 임의의 단어 'have'에 대해서 BiLSTM을 통해서 단어 표현 벡터를 얻는 과정을 보여준다. 우선, 단어 'have'를 'h', 'a', 'v', 'e'와 같이 글자 단위로 분리한다. 그리고 임베딩 층(Embedding layer)을 이용한 임베딩을 단어에 대해서 하는 것이 아니라 글자에 대해서 하게 된다. 다시 말해 글자를 임베딩한다. 그리고나서 정방향 LSTM은 단어 정방향으로 순차적으로 글자 임베딩 벡터를 읽는다. 반면, 역방향 LSTM은 단어의 역방향으로 순차적으로 글자 임베딩 벡터를 읽는다. 그리고 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태를 연결(concatenate)한다. 최종적으로 이렇게 얻은 벡터를 단어 'have'의 벡터로 사용한다. 그림에서는 글자 레벨 표현(Character-level representation)이라고 기재된 벡터에 해당된다.\n","\n","글자 임베딩을 워드 임베딩의 대체로서 쓸 수도 있겠지만, 이렇게 얻은 글자 임베딩을 워드 임베딩과 연결하여 신경망의 입력으로 사용함으로써 워드 임베딩과 함께 사용하기도 한다. 이에 대한 실습은 13챕터에서 진행해보겠다.\n","\n"]}]}