{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09) 임베딩 벡터의 시각화(Embedding Visualization).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPSlUEAJeConZ6zkqKuZDov"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2-P0rGGGmO9w"},"source":["## 09) 임베딩 벡터의 시각화(Embedding Visualization)\r\n","\r\n","구글은 임베딩 프로젝터(embedding projector)라는 데이터 시각화 도구를 지원한다. 이번 챕터에서는 임베딩 프로젝터를 사용하여 학습한 임베딩 벡터들을 시각화해보겠다.\r\n","\r\n","임베딩 프로젝터 논문 : https://arxiv.org/pdf/1611.05469v1.pdf\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"LfXLxcgmmQ4z"},"source":["### 1.워드 임베딩 모델로부터 2개의 tsv 파일 생성하기\r\n","\r\n","이번 실습에서는 학습한 임베딩 벡터들을 시각화해보겠다. 꼭 Word2Vec 등으로 학습해야하는 방법이 정해져 있지는 않고, GloVe 등 다른 방법으로 훈련되어 있어도 상관없다. 시각화를 위해서는 이미 모델을 학습하고, 파일로 저장되어져 있어야 한다. 모델이 저장되어져 있다면 아래 커맨드를 통해 시각화에 필요한 파일들을 생성할 수 있다.\r\n","\r\n","```\r\n","!python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름\r\n","```\r\n","\r\n","여기서는 편의를 위해 이전 챕터에서 학습하고 저장하는 실습까지 진행했던 영어 Word2Vec 모델인 'eng_w2v'를 재사용한다. eng_w2v라는 Word2Vec 모델이 이미 존재한다는 가정 하에 주피터 노트북에서 아래 커맨드를 수행한다.\r\n","\r\n","```\r\n","!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v\r\n","```\r\n","\r\n","커맨드를 수행하면 주피터 노트북이 시작되는 경로에 기존에 있던 eng_w2v 외에도 두 개의 파일이 생긴다.\r\n","\r\n","<img src = 'https://wikidocs.net/images/page/50704/eng_w2v.PNG'>\r\n","\r\n","새로 생긴 eng_w2v_metadata.tsv와 eng_w2v_tensor.tsv 이 두 개 파일이 임베딩 벡터 시각화를 위해 사용할 파일이다. 만약 eng_w2v 모델 파일이 아니라 다른 모델 파일 이름으로 실습을 진행하고 있다면, '모델 이름_metadata.tsv'와 '모델 이름_tensor.tsv'라는 파일이 생성된다고 이해하면 되겠다.\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KytWyrjdmQ67","executionInfo":{"status":"ok","timestamp":1615891291587,"user_tz":-540,"elapsed":18458,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"aa2df0b5-6f8b-42c0-9261-cdbea6055998"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saq87VRfmQ9T","executionInfo":{"status":"ok","timestamp":1615891313263,"user_tz":-540,"elapsed":687,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"19b4b044-fa7b-4bab-a763-001e7e06cbe3"},"source":["cd /content/gdrive/My Drive/02. 데이터"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/02. 데이터\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddeMMZO8mQ_J","executionInfo":{"status":"ok","timestamp":1615891317034,"user_tz":-540,"elapsed":716,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"ffe478c0-7c12-4897-d1ab-2d172e0f2f3c"},"source":["ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":[" _about.txt                             model.png\n","\u001b[0m\u001b[01;34m'강우예측AI 데이터'\u001b[0m/                    ner_dataset.csv\n"," best_model.h5                          pr_report.html\n"," eng_w2v                                ratings.txt\n"," fra-eng.zip                            spam.csv\n"," fra.txt                                train.txt\n","'폭격의 날개왕(통합).gdoc'             '폭격의 날개왕(통합).txt'\n"," IWSLT16.TED.tst2011.en-cs.en.xml       \u001b[01;34mwikiextractor\u001b[0m/\n"," kowiki-latest-pages-articles.xml.bz2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkBe2E12mRBY","executionInfo":{"status":"ok","timestamp":1615891356570,"user_tz":-540,"elapsed":7221,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"a43456ff-8d95-4bbd-fac4-030c4c2ff38d"},"source":["!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"],"execution_count":5,"outputs":[{"output_type":"stream","text":["2021-03-16 10:42:30,641 - word2vec2tensor - INFO - running /usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py --input eng_w2v --output eng_w2v\n","2021-03-16 10:42:30,641 - utils_any2vec - INFO - loading projection weights from eng_w2v\n","2021-03-16 10:42:33,264 - utils_any2vec - INFO - loaded (21613, 100) matrix from eng_w2v\n","2021-03-16 10:42:35,241 - word2vec2tensor - INFO - 2D tensor file saved to eng_w2v_tensor.tsv\n","2021-03-16 10:42:35,242 - word2vec2tensor - INFO - Tensor metadata file saved to eng_w2v_metadata.tsv\n","2021-03-16 10:42:35,245 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Se6QIbq7mRDo","executionInfo":{"status":"ok","timestamp":1615891369117,"user_tz":-540,"elapsed":702,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"373a8c3b-b91c-43f5-cd8e-b7fe14e6976b"},"source":["ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":[" _about.txt                         kowiki-latest-pages-articles.xml.bz2\n","\u001b[0m\u001b[01;34m'강우예측AI 데이터'\u001b[0m/                model.png\n"," best_model.h5                      ner_dataset.csv\n"," eng_w2v                            pr_report.html\n"," eng_w2v_metadata.tsv               ratings.txt\n"," eng_w2v_tensor.tsv                 spam.csv\n"," fra-eng.zip                        train.txt\n"," fra.txt                           '폭격의 날개왕(통합).txt'\n","'폭격의 날개왕(통합).gdoc'          \u001b[01;34mwikiextractor\u001b[0m/\n"," IWSLT16.TED.tst2011.en-cs.en.xml\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2QLIfkwsmRFs"},"source":["### 2.임베딩 프로젝터를 사용하여 시각화하기\r\n","\r\n","이제 구글의 임베딩 프로젝터를 사용해서 워드 임베딩 모델을 시각화해보겠다. 아래의 링크에 접속한다.\r\n","\r\n","링크 : https://projector.tensorflow.org/\r\n","\r\n","사이트에 접속해서 좌측 상단을 보면 Load라는 버튼이 있다.\r\n","\r\n","<img src = 'https://wikidocs.net/images/page/50704/embedding_projector.PNG'>\r\n","\r\n","Load라는 버튼을 누르면 아래와 같은 창이 뜨는데 총 두 개의 Choose file 버튼이 있다.\r\n","\r\n","<img src = 'https://wikidocs.net/images/page/50704/embedding_projector2.PNG'>\r\n","\r\n","위에 있는 Choose file 버튼을 누르고 eng_w2v_tensor.tsv 파일을 업로드하고, 아래에 있는 Choose file 버튼을 누르고 eng_w2v_metadata.tsv 파일을 업로드한다. 두 파일을 업로드하면 임베딩 프로젝터에 학습했던 워드 임베딩 모델이 시각화된다.\r\n","\r\n","<img src = 'https://wikidocs.net/images/page/50704/man.PNG'>\r\n","\r\n","그 후에는 임베딩 프로젝터의 다양한 기능을 사용할 수 있다. 예를 들어 임베딩 프로젝터는 복잡한 데이터를 차원을 축소하여 시각화할 수 있도록 도와주는 PCA, t-SNE 등을 제공한다. 여기서는 자세한 기능에 대한 설명은 생략하겠다. 위의 그림은 'man'이라는 단어를 선택하고, 코사인 유사도를 기준으로 가장 유사한 상위 10개 벡터들을 표시해본 그림이다.\r\n","\r\n","\r\n","\r\n"]}]}