{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06) 패스트텍스트(FastText).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMsHsS6wxeJZ0D88yswhj8v"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m0veuUjko6h0"},"source":["## 06) 패스트텍스트(FastText)\r\n","\r\n","단어를 벡터로 만드는 또 다른 방법으로는 페이스북에서 개발한 FastText가 있다. Word2Vec 이후에 나온 것이기 때문에, 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있다. Word2Vec과 FastText와의 가장 큰 차이점이라면 Word2Vec은 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주한다. 즉, 내부 단어(subword)를 고려하여 학습한다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"s7x0YI3tpBJw"},"source":["### 1.내부 단어(subword)의 학습\r\n","\r\n","FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급한다. n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정된다. 예를 들어서 n을 3으로 잡은 트라이그램(tri-gram)의 경우, apple은 app, ppl, ple로 분리하고 이들을 벡터로 만든다. 더 정확히는 시작과 끝을 의미하는 <, >를 도입하여 아래의 5개 내부 단어(subword) 토큰을 벡터로 만든다.\r\n","\r\n","```\r\n","# n = 3인 경우\r\n","<ap, app, ppl, ple, le> \r\n","```\r\n","\r\n","그리고 여기에 추가적으로 하나를 더 벡터화하는데, 기존 단어에 <와 >를 붙인 토큰이다.\r\n","\r\n","```\r\n","# 특별 토큰\r\n","<apple>\r\n","```\r\n","\r\n","다시 말해 n = 3인 경우, FastText는 단어 apple에 대해서 다음의 6개의 토큰을 벡터화하는 것이다.\r\n","\r\n","```\r\n","# n = 3인 경우\r\n","<ap, app, ppl, ple, le>, <apple>\r\n","```\r\n","\r\n","그런데 실제 사용할 때는 n의 최소값과 최대값으로 범위를 설정할 수 있는데, 기본값으로는 각각 3과 6으로 설정되어져 있다. 다시 말해 최소값 = 3, 최대값 = 6인 경우라면, 단어 apple에 대해서 FastText는 아래 내부 단어들을 벡터화한다.\r\n","\r\n","```\r\n","# n = 3 ~ 6인 경우\r\n","<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>\r\n","```\r\n","\r\n","여기서 내부 단어들을 벡터화한다는 의미는 저 단어들에 대해서 Word2Vec을 수행한다는 의미이다. 위와 같이 내부 단어들의 벡터값을 얻었다면, 단어 apple의 벡터값은 저 위 벡터값들의 총 합으로 구성한다.\r\n","\r\n","```\r\n","apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>\r\n","```\r\n","\r\n","그리고 이런 방법은 Word2Vec에서는 얻을 수 없었던 강점을 가진다.\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"xFNTOGicpBL3"},"source":["### 2.모르는 단어(Out Of Vocabulary, OOV)에 대한 대응\r\n","\r\n","FastText의 인공 신경망을 학습한 후에는 데이터셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩이 된다. 이렇게 되면 장점은 데이터셋만 충분하다면 위와 같은 내부 단어(Subword)를 통해 모르는 단어(Out Of Vocabulary, OOV)에 대해서도 다른 단어와의 유사도를 계산할 수 있다는 점이다.\r\n","\r\n","가령, FastText에서 birthplace(출생지)란 단어를 학습하지 않은 상태라고 해보자. 하지만 다른 단어에서 birth와 place라는 내부 단어가 있었다면, FastText는 birthplace의 벡터를 얻을 수 있다. 이는 모르는 단어에 제대로 대처할 수 없는 Word2Vec, GloVe와는 다른 점이다. \r\n"]},{"cell_type":"markdown","metadata":{"id":"kXdtEcHOpBOC"},"source":["### 3.단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응\r\n","\r\n","Word2Vec의 경우에는 등장 빈도 수가 적은 단어(rare word)에 대해서는 임베딩의 정확도가 높지 않다는 단점이 있었다. 참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않는 경우이다.\r\n","\r\n","하지만 FastText의 경우, 만약 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교하여 비교적 높은 임베딩 벡터값을 얻는다.\r\n","\r\n","FastText가 노이즈가 많은 코퍼스에서 강점을 가진 것 또한 이와 같은 이유이다. 모든 훈련 코퍼스에 오타(Typo)나 맞춤법이 틀린 단어가 없으면 이상적이겠지만, 실제 많은 비정형 데이터에는 오타가 섞여있다. 그리고 오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 된다. 즉, Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만 FastText는 이에 대해서도 일정 수준의 성능을 보인다.\r\n","\r\n","예를 들어 단어 apple과 오타로 p를 한 번 더 입력한 appple의 경우에는 실제로 많은 개수의 동일한 n-gram을 가질 것이다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"nxKUXnq8pBPw"},"source":["### 4.실습으로 비교하는 Word2Vec vs FastText\r\n","\r\n","간단한 실습을 통해 Word2Vec와 FastText의 차이를 비교해보도록 하겠다. 단, 사용하는 코드는 Word2Vec을 실습하기 위해 사용했던 이전 챕터의 동일한 코드를 사용한다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"KiDA2TdSpBRy"},"source":["#### 1) Word2Vec\r\n","\r\n","우선, 이전 챕터(https://wikidocs.net/50739)의 전처리 코드와 Word2Vec 학습 코드를 그대로 수행했음을 가정하겠다.\r\n","\r\n","===== 이전 챕터 전처리 코드 시작"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnmGLqlipBT1","executionInfo":{"status":"ok","timestamp":1615876524227,"user_tz":-540,"elapsed":2148,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"f8fa8cea-7118-47a0-a0f2-56a40fe5096d"},"source":["import nltk\r\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"_WfQcOmKvrtZ","executionInfo":{"status":"ok","timestamp":1615876526506,"user_tz":-540,"elapsed":580,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import urllib.request\r\n","import zipfile\r\n","from lxml import etree\r\n","import re\r\n","from nltk.tokenize import word_tokenize, sent_tokenize"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmGnpvoYvrvo","executionInfo":{"status":"ok","timestamp":1615876534652,"user_tz":-540,"elapsed":1412,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"b7263dcf-2866-4006-cd67-7266f6740e5d"},"source":["# 데이터 다운로드\r\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\r\n","                           filename=\"ted_en-20160408.xml\")\r\n"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7f859acaa3d0>)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"KBvRT9ERvrxr","executionInfo":{"status":"ok","timestamp":1615876588969,"user_tz":-540,"elapsed":47425,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\r\n","target_text = etree.parse(targetXML)\r\n","\r\n","# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\r\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\r\n","\r\n","# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\r\n","# 해당 코드는 괄호로 구성된 내용을 제거.\r\n","content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\r\n","\r\n","# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\r\n","sent_text = sent_tokenize(content_text)\r\n","\r\n","# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\r\n","normalized_text = []\r\n","for string in sent_text:\r\n","     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\r\n","     normalized_text.append(tokens)\r\n","\r\n","# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\r\n","result = [word_tokenize(sentence) for sentence in normalized_text]\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2RL8mb9vrzp","executionInfo":{"status":"ok","timestamp":1615876588970,"user_tz":-540,"elapsed":41490,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"87441911-e7b8-4c81-e2f4-88f54c47f1c6"},"source":["print('총 샘플의 개수 : {}'.format(len(result)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["총 샘플의 개수 : 273424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Za70zbj8wHTJ","executionInfo":{"status":"ok","timestamp":1615876649477,"user_tz":-540,"elapsed":603,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"a4590fa2-0107-4204-8a4c-640992bf7531"},"source":["# 샘플 3개만 출력\r\n","for line in result[:3]:\r\n","    print(line)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n","['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n","['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-oudJ_ciwFAx","executionInfo":{"status":"ok","timestamp":1615876645692,"user_tz":-540,"elapsed":34889,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["from gensim.models import Word2Vec\r\n","model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZ7G0-wGwFC7","executionInfo":{"status":"ok","timestamp":1615876651695,"user_tz":-540,"elapsed":600,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"3fd841d4-dc58-44ac-bc38-1ea98d712338"},"source":["model_result = model.wv.most_similar(\"man\")\r\n","print(model_result)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[('woman', 0.8692293763160706), ('guy', 0.8290320038795471), ('boy', 0.7834029793739319), ('girl', 0.7682618498802185), ('lady', 0.7649726867675781), ('gentleman', 0.7627958655357361), ('soldier', 0.7332321405410767), ('kid', 0.7149725556373596), ('poet', 0.6882311105728149), ('friend', 0.6714836955070496)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EK2EYpVpwTts"},"source":["===== 이전 챕터 전처리 코드 완료"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"OfwBNHDqwFFQ","executionInfo":{"status":"error","timestamp":1615876654737,"user_tz":-540,"elapsed":773,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"ef5482da-2f19-4acf-8365-0060b61b76ce"},"source":["model.wv.most_similar(\"electrofishing\")"],"execution_count":11,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f9544fe3d5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""]}]},{"cell_type":"markdown","metadata":{"id":"fIvgY_1HpBV6"},"source":["입력 단어에 대해서 유사한 단어를 찾아내는 코드에 이번에는 electrofishing이라는 단어를 넣어보겠다.\r\n","\r\n","``` python\r\n","model.wv.most_similar(\"electrofishing\")\r\n","```\r\n","\r\n","해당 코드는 작동하지 않고, 이런 에러를 발생시킨다.\r\n","\r\n","```\r\n","KeyError: \"word 'electrofishing' not in vocabulary\"\r\n","```\r\n","\r\n","에러 메시지는 단어 집합(Vocabulary)에 electrofishing이 존재하지 않는다고 한다. 이처럼 Word2Vec은 학습 데이터에 존재하지 않는 단어. 즉, 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않기 때문에 단어의 유사도를 계산할 수 없다.\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"sgtAw7YWuu0n"},"source":["#### 2) FastText\r\n","\r\n","이번에는 전처리 코드는 그대로 사용하고, Word2Vec 학습 코드만 FastText 학습 코드로 변경하여 실행해보자.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"QgoXEP0UvNJq","executionInfo":{"status":"ok","timestamp":1615876842980,"user_tz":-540,"elapsed":151945,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["from gensim.models import FastText\r\n","\r\n","model = FastText(result, size = 100, window = 5, min_count = 5, workers = 4, sg = 1)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4TpKMNfvUYg"},"source":["학습이 진행되었다면, 이제 electrofishing에 대해서 유사 단어를 찾아보도록 하겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Coum2_b4vbqE","executionInfo":{"status":"ok","timestamp":1615876842981,"user_tz":-540,"elapsed":140946,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"1856974f-2405-4eaf-fb8d-c25b6f4edb33"},"source":["model.wv.most_similar(\"electrofishing\")"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('electrolux', 0.8098731637001038),\n"," ('electro', 0.798114538192749),\n"," ('electrolyte', 0.7902266979217529),\n"," ('electroshock', 0.7770769596099854),\n"," ('electrochemical', 0.7732920050621033),\n"," ('electric', 0.7677932381629944),\n"," ('electrons', 0.7607604265213013),\n"," ('electron', 0.7589715719223022),\n"," ('fishing', 0.7549363374710083),\n"," ('electrogram', 0.7521032094955444)]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"66g1lj-BvcqN"},"source":["Word2Vec은 학습하지 않은 단어에 대해서 유사한 단어를 찾아내지 못 했지만, FastText는 유사한 단어를 계산해서 출력하고 있음을 볼 수 있다.\r\n"]}]}