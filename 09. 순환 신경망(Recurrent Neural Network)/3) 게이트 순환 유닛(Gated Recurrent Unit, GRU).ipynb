{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09. 순환 신경망 / 3) 게이트 순환 유닛.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMkHKqXcCViMjTxrca2AQ+u"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IVZI9942MRpa"},"source":["## 3) 게이트 순환 유닛(Gated Recurrent Unit, GRU)\r\n","\r\n","GRU(Gated Recurrent Unit)는 2014년 뉴욕대학교 조경현 교수가 집필한 논문에서 제안되었다. GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰다."]},{"cell_type":"markdown","metadata":{"id":"mh6UCHT5MUK3"},"source":["### 1.GRU(Gated Recurrent Unit)\r\n","\r\n","LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재한다. GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있다.\r\n","\r\n","**GRU 그림은 현재 다른 챕터와 일관성이 맞지 않은 예전 버전 그림으로 향후 수정 예정이다.**\r\n","\r\n","<img src = 'https://wikidocs.net/images/page/22889/GRU.PNG' width = 60%>\r\n","\r\n","반드시 LSTM 대신 GRU를 사용하는 것이 좋지는 않다. GRU와 LSTM 중 어떤 것이 모델의 성능면에서 더 낫다라고 단정지어 말할 수 없으며, 기존에 LSTM을 사용하면서 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU로 바꿔서 사용할 필요는 없다.\r\n","\r\n","데이터 양이 적을 때는, 매개변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문이다.\r\n","\r\n","<br>\r\n","\r\n","Massive Exploration of Neural Machine Translation Architectures, \r\n","\r\n","2017 논문 中\r\n","\r\n","In our experiments, LSTM cells consistently outperformed GRU cells"]},{"cell_type":"markdown","metadata":{"id":"cAKvYdxIMUNl"},"source":["### 2.케라스에서의 GRU(Gated Recurrent Unit)\r\n","\r\n","케라스에서는 역시 GRU에 대한 구현을 지원한다. 사용 방법은 SimpleRNN이나 LSTM과 같다.\r\n","\r\n","``` python\r\n","# 실제 GRU 은닉층을 추가하는 코드.\r\n","model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))\r\n","```\r\n","\r\n"]}]}