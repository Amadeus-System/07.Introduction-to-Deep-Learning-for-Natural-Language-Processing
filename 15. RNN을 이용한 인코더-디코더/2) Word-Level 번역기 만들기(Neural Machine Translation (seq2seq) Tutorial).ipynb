{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"15. RNN을 이용한 인코더-디코더 / 2) Word-Level 번역기 만들기.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOLZJqSCgW5WDWLdwURW34h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sP68McA_Qppb"},"source":["## 2) Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial)\r\n","\r\n","이전 챕터에서 seq2seq를 사용하여 글자 레벨(character-level)의 기계 번역기를 만들었다. 이번 챕터에서는 단어 레벨(Word-level)의 기계 번역기를 만들어보자. 모델 아키텍처는 이전 챕터와 거의 동일하지만, 단어 레벨을 수행하는 만큼 추가적인 전처리와 임베딩 층(Embedding layer), 그리고 추가적인 후처리 작업이 필요하다.\r\n","\r\n","이번 챕터는 이전 챕터의 내용을 이해했다는 가정 하에 모델에 대한 설명을 자세하게 하지 않는다.\r\n"]},{"cell_type":"markdown","metadata":{"id":"zrRBGqOLQw_a"},"source":["### 1.데이터 로드 및 전처리\r\n","\r\n","필요한 도구들을 임포트한다.\r\n"]},{"cell_type":"code","metadata":{"id":"yIVXfxIDQxBo","executionInfo":{"status":"ok","timestamp":1610607539037,"user_tz":-540,"elapsed":2346,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import numpy as np\r\n","import pandas as pd\r\n","import re\r\n","import shutil\r\n","import os\r\n","import unicodedata\r\n","import urllib3\r\n","import zipfile\r\n","import tensorflow as tf\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6toJXSpRQxD_"},"source":["데이터를 로드한다. 데이터는 이전 챕터와 동일한 데이터를 사용할 예정이다. 이에 데이터 구성에 대한 설명은 생략한다.\r\n"]},{"cell_type":"code","metadata":{"id":"egot5YgtQxGF","executionInfo":{"status":"ok","timestamp":1610607539708,"user_tz":-540,"elapsed":3010,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["http = urllib3.PoolManager()\r\n","url ='http://www.manythings.org/anki/fra-eng.zip'\r\n","filename = 'fra-eng.zip'\r\n","path = os.getcwd()\r\n","zipfilename = os.path.join(path, filename)\r\n","with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \r\n","    shutil.copyfileobj(r, out_file)\r\n","\r\n","with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\r\n","    zip_ref.extractall(path)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vKRZyY2QxIN","executionInfo":{"status":"ok","timestamp":1610607539709,"user_tz":-540,"elapsed":3004,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"848ff428-bea2-4e7d-f407-046536b7a7d2"},"source":["ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["_about.txt  fra-eng.zip  fra.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WnzZeTMGQxKb"},"source":["이번 챕터에서는 총 33,000개의 샘플을 사용할 예정이다. 이 값을 변수에 지정한다.\r\n"]},{"cell_type":"code","metadata":{"id":"4c4ozzycQxMq","executionInfo":{"status":"ok","timestamp":1610607539710,"user_tz":-540,"elapsed":2999,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["num_samples = 33000"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_XXl6DhQxPD"},"source":["전처리 함수들을 구현한다."]},{"cell_type":"code","metadata":{"id":"OYsdp5jFQxRd","executionInfo":{"status":"ok","timestamp":1610607539710,"user_tz":-540,"elapsed":2994,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def unicode_to_ascii(s):\r\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\r\n","    "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"sl_HWaAuUCaS","executionInfo":{"status":"ok","timestamp":1610607539711,"user_tz":-540,"elapsed":2990,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def preprocess_sentence(sent):\r\n","    # 위에서 구현한 함수를 내부적으로 호출\r\n","    sent = unicode_to_ascii(sent.lower())\r\n","\r\n","    # 단어와 구두점 사이에 공백을 만든다.\r\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\r\n","    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\r\n","\r\n","    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환한다.\r\n","    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\r\n","\r\n","    sent = re.sub(r\"\\s+\", \" \", sent)\r\n","    return sent"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0R0oOBceUeCk"},"source":["구현한 전처리 함수들을 임의의 문장을 입력으로 테스트해보자.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0by4yj2LUxpX","executionInfo":{"status":"ok","timestamp":1610607539711,"user_tz":-540,"elapsed":2984,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"a9fbb927-1717-4c70-867a-26dfe616e65e"},"source":["# 전처리 테스트\r\n","en_sent = u\"Have you had dinner?\"\r\n","fr_sent = u\"Avez-vous déjà diné?\"\r\n","print(preprocess_sentence(en_sent))\r\n","print(preprocess_sentence(fr_sent).encode('utf-8'))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["have you had dinner ?\n","b'avez vous deja dine ?'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kulUOqM0VAfQ"},"source":["전체 데이터에서 33,000개의 샘플만 불러오되, 모든 전처리를 수행하는 함수를 만든다. 또한 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로, 훈련 시 사용할 디코더의 입력 시퀀스와 실제값에 해당되는 출력 시퀀스를 따로 분리하여 저장한다. 입력 시퀀스에는 시작을 의미하는 토큰인 <(sos)>를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 <(eos)>를 추가한다.\r\n"]},{"cell_type":"code","metadata":{"id":"2Ja-7eM3VVGd","executionInfo":{"status":"ok","timestamp":1610607539712,"user_tz":-540,"elapsed":2979,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def load_preprocessed_data():\r\n","    encoder_input, decoder_input, decoder_target = [], [], []\r\n","\r\n","    with open('fra.txt', 'r') as lines:\r\n","        for i, line in enumerate(lines):\r\n","\r\n","            # source 데이터와 target 데이터 분리\r\n","            src_line, tar_line, _ = line.strip().split('\\t')\r\n","\r\n","            # source 데이터 전처리\r\n","            src_line_input = [w for w in preprocess_sentence(src_line).split()]\r\n","\r\n","            # target 데이터 전처리\r\n","            tar_line = preprocess_sentence(tar_line)\r\n","            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]\r\n","            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]\r\n","\r\n","            encoder_input.append(src_line_input)\r\n","            decoder_input.append(tar_line_input)\r\n","            decoder_target.append(tar_line_target)\r\n","\r\n","            if i == num_samples - 1:\r\n","                break\r\n","            \r\n","    return encoder_input, decoder_input, decoder_target"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y68KxxxsVVsI"},"source":["이렇게 얻은 3개의 데이터셋은 인코더의 입력, 디코더의 입력, 디코더의 실제값을 상위 5개 샘플만 출력해보자.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ix56lxHtVVut","executionInfo":{"status":"ok","timestamp":1610607540883,"user_tz":-540,"elapsed":4143,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"121af98c-9515-4cf1-ff30-4ff7b2d6c7b0"},"source":["sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\r\n","\r\n","print(sents_en_in[:5])\r\n","print(sents_fra_in[:5])\r\n","print(sents_fra_out[:5])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[['go', '.'], ['hi', '.'], ['hi', '.'], ['run', '!'], ['run', '!']]\n","[['<sos>', 'va', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.'], ['<sos>', 'cours', '!'], ['<sos>', 'courez', '!']]\n","[['va', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>'], ['cours', '!', '<eos>'], ['courez', '!', '<eos>']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WdXSjzleWXLh"},"source":["이제 케라스 토크나이저를 통해 단어 집합을 생성하고, 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩 과정을 거친다.\r\n"]},{"cell_type":"code","metadata":{"id":"nVUIm8__WXOJ","executionInfo":{"status":"ok","timestamp":1610607541662,"user_tz":-540,"elapsed":4916,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["tokenizer_en = Tokenizer(filters = '', lower = False)\r\n","tokenizer_en.fit_on_texts(sents_en_in)\r\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\r\n","\r\n","tokenizer_fra = Tokenizer(filters = '', lower = False)\r\n","tokenizer_fra.fit_on_texts(sents_fra_in)\r\n","tokenizer_fra.fit_on_texts(sents_fra_out)\r\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\r\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbfN3aEPWXQU"},"source":["이어서 패딩을 수행한다.\r\n"]},{"cell_type":"code","metadata":{"id":"5BBI0KlpWXS0","executionInfo":{"status":"ok","timestamp":1610607541994,"user_tz":-540,"elapsed":5243,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["encoder_input = pad_sequences(encoder_input, padding = 'post')\r\n","decoder_input = pad_sequences(decoder_input, padding = 'post')\r\n","decoder_target = pad_sequences(decoder_target, padding = 'post')"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OJoF0b-kWXVI"},"source":["이렇게 얻은 데이터의 크기(shape)를 확인한다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMgvkvk1WXXo","executionInfo":{"status":"ok","timestamp":1610607541994,"user_tz":-540,"elapsed":5237,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"6853acea-bebd-4b25-e59a-4d68b56be6ce"},"source":["print(encoder_input.shape)\r\n","print(decoder_input.shape)\r\n","print(decoder_target.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["(33000, 8)\n","(33000, 16)\n","(33000, 16)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OiE-Yd6VWXaI"},"source":["샘플은 총 33,000개 존재하며 영어 문장의 길이는 8, 프랑스어 문장의 길이는 16이다. 단어 집합의 크기를 정의한다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14nucVlpWXcl","executionInfo":{"status":"ok","timestamp":1610607541995,"user_tz":-540,"elapsed":5231,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"2a91a9cc-4b25-4b18-bf08-357242557cde"},"source":["src_vocab_size = len(tokenizer_en.word_index) + 1\r\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\r\n","print('영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}'.format(src_vocab_size, tar_vocab_size))\r\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["영어 단어 집합의 크기 : 4678, 프랑스어 단어 집합의 크기 : 8032\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dCXLreSQWXeu"},"source":["단어 집합의 크기는 각각 4,678개와 8,032개이다. 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어준다. 이들은 훈련을 마치고 예측 과정과 실제값과 결과를 비교하는 경우에 사용된다.\r\n"]},{"cell_type":"code","metadata":{"id":"beWhUKHQWXhA","executionInfo":{"status":"ok","timestamp":1610607541995,"user_tz":-540,"elapsed":5225,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["src_to_index = tokenizer_en.word_index\r\n","index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\r\n","\r\n","tar_to_index = tokenizer_fra.word_index # 훈련 후 예측 과정에서 사용\r\n","index_to_tar = tokenizer_fra.index_word # 훈련 후 결과 비교할 때 사용"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AkE43fxIWXjf"},"source":["이제 테스트 데이터를 분리할 차례이다. 테스트 데이터를 분리하기 전에, 적절한 분포를 갖도록 데이터를 섞어주는 과정을 진행한다. 이를 위해서 우선 순서가 섞인 정수 시퀀스 리스트를 만든다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FV9JWTGFWXlx","executionInfo":{"status":"ok","timestamp":1610607541995,"user_tz":-540,"elapsed":5218,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"d61d1a45-fd28-4655-f3dc-2ae6b92ef5ba"},"source":["indices = np.arange(encoder_input.shape[0])\r\n","np.random.shuffle(indices)\r\n","print(indices)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[14723   408 10528 ... 21442  1069 12502]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PAGkGPh3WXoO"},"source":["이를 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞이게 된다.\r\n"]},{"cell_type":"code","metadata":{"id":"ERJT-jDZVVxW","executionInfo":{"status":"ok","timestamp":1610607541996,"user_tz":-540,"elapsed":5213,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["encoder_input = encoder_input[indices]\r\n","decoder_input = decoder_input[indices]\r\n","decoder_target = decoder_target[indices]"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAVkPS7aVVz6"},"source":["임의로 30,997번째 샘플을 출력해보자. 이때, decoder_input과 decoder_target은 데이터의 구조상으로 앞에 붙은 <(sos)> 토큰과 뒤에 붙은 <(eos)> 토큰을 제외하면 동일한 정수 시퀀스를 가져야하므로 이를 확인해주면 된다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I71VWTXxVV1y","executionInfo":{"status":"ok","timestamp":1610607541996,"user_tz":-540,"elapsed":5206,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"051a08b1-d0ad-43e7-bc86-e4b05d1c3b55"},"source":["encoder_input[30997]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 14,   8, 190,  17,  63,   1,   0,   0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JL1H9QLHVV4O","executionInfo":{"status":"ok","timestamp":1610607541996,"user_tz":-540,"elapsed":5198,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"f16cb02f-4049-4093-8349-16a1bb02d676"},"source":["decoder_input[30997]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  2,   9,   5, 943,  16, 274,   1,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtIxMTOvVV6j","executionInfo":{"status":"ok","timestamp":1610607541997,"user_tz":-540,"elapsed":5192,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"7b32fe50-b3d1-4f60-a4fd-7e68b4d5bedf"},"source":["decoder_target[30997]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  9,   5, 943,  16, 274,   1,   3,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"tOvm2TUcVV8_"},"source":["저자의 경우 18, 5, 16, 173, 1이라는 동일 시퀀스를 확인했다. 이제 훈련 데이터의 10%를 테스트 데이터로 분리하겠다.\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMfMUiqyVV_T","executionInfo":{"status":"ok","timestamp":1610607541997,"user_tz":-540,"elapsed":5184,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"3c02d34f-4f69-43ef-aa02-84bbb59c59be"},"source":["n_of_val = int(33000 * 0.1)\r\n","print(n_of_val)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["3300\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YStfRpyjVWBT"},"source":["33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용한다. \r\n"]},{"cell_type":"code","metadata":{"id":"zEXgXUx3VWDp","executionInfo":{"status":"ok","timestamp":1610607541998,"user_tz":-540,"elapsed":5179,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["encoder_input_train = encoder_input[:-n_of_val]\r\n","decoder_input_train = decoder_input[:-n_of_val]\r\n","decoder_target_train = decoder_target[:-n_of_val]\r\n","\r\n","encoder_input_test = encoder_input[-n_of_val:]\r\n","decoder_input_test = decoder_input[-n_of_val:]\r\n","decoder_target_test = decoder_target[-n_of_val:]"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f08XOjEsVWGW"},"source":["훈련 데이터와 테스트 데이터의 크기(shape)를 출력해보자.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJ_guJxwb2xm","executionInfo":{"status":"ok","timestamp":1610607541998,"user_tz":-540,"elapsed":5173,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"541e019e-352c-4bd1-dc0c-21349294ead4"},"source":["print(encoder_input_train.shape)\r\n","print(decoder_input_train.shape)\r\n","print(decoder_target_train.shape)\r\n","\r\n","print(encoder_input_test.shape)\r\n","print(decoder_input_test.shape)\r\n","print(decoder_target_test.shape)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["(29700, 8)\n","(29700, 16)\n","(29700, 16)\n","(3300, 8)\n","(3300, 16)\n","(3300, 16)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c0XE3jMScHWQ"},"source":["훈련 데이터의 샘플은 29,700개, 테스트 데이터의 샘플은 3,300개가 존재한다. 이제 모델을 설계한다. "]},{"cell_type":"markdown","metadata":{"id":"hqc-svXLcSPV"},"source":["### 2.기계 번역기 만들기\r\n","\r\n","모델 설계를 위해 필요한 도구들을 임포트한다.\r\n"]},{"cell_type":"code","metadata":{"id":"g3wPpspGcVn9","executionInfo":{"status":"ok","timestamp":1610607541999,"user_tz":-540,"elapsed":5168,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\r\n","from tensorflow.keras.models import Model"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0Drz8hRcVqO"},"source":["임베딩 벡터와 LSTM의 은닉 상태의 크기를 특정 크기로 고정하고자 한다. 여기서는 50을 사용한다.\r\n"]},{"cell_type":"code","metadata":{"id":"LU29LneIcVvG","executionInfo":{"status":"ok","timestamp":1610607541999,"user_tz":-540,"elapsed":5164,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["latent_dim = 50"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ydPjN3jcVxN"},"source":["인코더를 설계한다. Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행한다.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"-AHtcTk9cV0l","executionInfo":{"status":"ok","timestamp":1610607548888,"user_tz":-540,"elapsed":12048,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 인코더\r\n","encoder_inputs = Input(shape = (None, ))\r\n","enc_emb = Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\r\n","enc_masking = Masking(mask_value = 0.0)(enc_emb) # 패딩 0은 연산에서 제외\r\n","\r\n","encoder_lstm = LSTM(latent_dim, return_state = True) # 상태값 리턴을 위해 return_state는 True\r\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\r\n","encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장\r\n"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmO4ALENcV2R"},"source":["이제 디코더를 설계한다.\r\n"]},{"cell_type":"code","metadata":{"id":"sp7Bt0ZRcV-Q","executionInfo":{"status":"ok","timestamp":1610607549591,"user_tz":-540,"elapsed":12747,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 디코더\r\n","decoder_inputs = Input(shape = (None, ))\r\n","dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\r\n","dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\r\n","dec_masking = Masking(mask_value = 0.0)(dec_emb)\r\n","\r\n","# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences = True\r\n","decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True)\r\n","\r\n","# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\r\n","decoder_outputs, _, _ = decoder_lstm(dec_masking,\r\n","                                     initial_state = encoder_states)\r\n","\r\n","# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\r\n","decoder_dense = Dense(tar_vocab_size, activation = 'softmax')\r\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"smr_tXGpcWA4"},"source":["모델의 입력과 출력을 정의함으로써 모델을 정의한다.\r\n"]},{"cell_type":"code","metadata":{"id":"69ttpCNZcWDN","executionInfo":{"status":"ok","timestamp":1610607549592,"user_tz":-540,"elapsed":12744,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5H7d9PddcWOF"},"source":["seq2seq의 디코더는 기본적으로 각각의 시점(timestep)에 대해서 다중 클래스 분류 문제를 풀고 있다. 매 시점마다 프랑스어 단어 집합의 크기의 선택지에서 단어를 1개 선택하여 이를 이번 시점에서 예측한 단어로 택한다. 다중 클래스 분류 문제이므로 위의 설계에서 출력층으로 소프트맥스 함수를 사용했다. 이 경우 손실 함수를 지금까지 categorical_crossentropy를 사용해왔다.\r\n","\r\n","categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 한다. 그런데 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태이다. 원-핫 인코딩을 하지 않은 상태로, 정수 레이블에 대해서 다중 클래스 분류 문제를 풀고자 하는 경우에는 categorical_crossentropy 함수가 아니라 sparse_categorical_crossentropy를 사용하면 된다. 이는 케라스에서 규정한 약속이다.\r\n"]},{"cell_type":"code","metadata":{"id":"crqLEOBTcWRb","executionInfo":{"status":"ok","timestamp":1610607549592,"user_tz":-540,"elapsed":12739,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["model.compile(optimizer = 'rmsprop',\r\n","              loss = 'sparse_categorical_crossentropy',\r\n","              metrics = ['acc'])\r\n"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_PtUZo68cWYq"},"source":["모델의 파라미터를 확인해보자.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8r4luA07rmwB","executionInfo":{"status":"ok","timestamp":1610607549593,"user_tz":-540,"elapsed":12736,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"bc509bdb-bc74-426e-a6f0-4c7f280d07b3"},"source":["model.summary()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, None, 50)     233900      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, None, 50)     401600      input_2[0][0]                    \n","__________________________________________________________________________________________________\n","masking (Masking)               (None, None, 50)     0           embedding[0][0]                  \n","__________________________________________________________________________________________________\n","masking_1 (Masking)             (None, None, 50)     0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     [(None, 50), (None,  20200       masking[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, None, 50), ( 20200       masking_1[0][0]                  \n","                                                                 lstm[0][1]                       \n","                                                                 lstm[0][2]                       \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, None, 8032)   409632      lstm_1[0][0]                     \n","==================================================================================================\n","Total params: 1,085,532\n","Trainable params: 1,085,532\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cSwt3BTrrnt_"},"source":["현재 모델의 총 파라미터 개수는 1,082,972개이다. 이제 모델을 훈련한다. 128개의 배치 크기로 총 50 에포크 학습한다. 테스트 데이터를 검증 데이터로 사용하여 훈련이 제대로 되고 있는지 모니터링 하겠다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBoHg1Qmr3YV","executionInfo":{"status":"ok","timestamp":1610608002497,"user_tz":-540,"elapsed":465638,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"8ff695ad-7c38-456b-8c5b-102f1ae4b0ef"},"source":["hist = model.fit(x = [encoder_input_train, decoder_input_train], \r\n","                 y = decoder_target_train,\r\n","                 validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\r\n","                 batch_size = 128,\r\n","                 epochs = 50)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","233/233 [==============================] - 23s 47ms/step - loss: 4.8117 - acc: 0.5889 - val_loss: 1.9102 - val_acc: 0.7046\n","Epoch 2/50\n","233/233 [==============================] - 9s 39ms/step - loss: 1.7848 - acc: 0.7229 - val_loss: 1.6320 - val_acc: 0.7391\n","Epoch 3/50\n","233/233 [==============================] - 9s 39ms/step - loss: 1.5582 - acc: 0.7461 - val_loss: 1.5340 - val_acc: 0.7505\n","Epoch 4/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.4569 - acc: 0.7611 - val_loss: 1.4327 - val_acc: 0.7717\n","Epoch 5/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.3612 - acc: 0.7804 - val_loss: 1.3671 - val_acc: 0.7830\n","Epoch 6/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.2855 - acc: 0.7900 - val_loss: 1.3097 - val_acc: 0.7927\n","Epoch 7/50\n","233/233 [==============================] - 9s 39ms/step - loss: 1.2305 - acc: 0.7974 - val_loss: 1.2626 - val_acc: 0.7971\n","Epoch 8/50\n","233/233 [==============================] - 9s 39ms/step - loss: 1.1835 - acc: 0.8042 - val_loss: 1.2261 - val_acc: 0.8050\n","Epoch 9/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.1398 - acc: 0.8112 - val_loss: 1.1912 - val_acc: 0.8107\n","Epoch 10/50\n","233/233 [==============================] - 9s 37ms/step - loss: 1.1001 - acc: 0.8179 - val_loss: 1.1677 - val_acc: 0.8128\n","Epoch 11/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.0772 - acc: 0.8212 - val_loss: 1.1494 - val_acc: 0.8146\n","Epoch 12/50\n","233/233 [==============================] - 9s 37ms/step - loss: 1.0465 - acc: 0.8250 - val_loss: 1.1240 - val_acc: 0.8190\n","Epoch 13/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.0242 - acc: 0.8282 - val_loss: 1.1134 - val_acc: 0.8209\n","Epoch 14/50\n","233/233 [==============================] - 9s 38ms/step - loss: 1.0017 - acc: 0.8310 - val_loss: 1.0919 - val_acc: 0.8236\n","Epoch 15/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.9858 - acc: 0.8329 - val_loss: 1.0716 - val_acc: 0.8268\n","Epoch 16/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.9598 - acc: 0.8362 - val_loss: 1.0672 - val_acc: 0.8269\n","Epoch 17/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.9476 - acc: 0.8373 - val_loss: 1.0493 - val_acc: 0.8281\n","Epoch 18/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.9292 - acc: 0.8401 - val_loss: 1.0325 - val_acc: 0.8306\n","Epoch 19/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.9149 - acc: 0.8421 - val_loss: 1.0323 - val_acc: 0.8313\n","Epoch 20/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.8924 - acc: 0.8447 - val_loss: 1.0069 - val_acc: 0.8359\n","Epoch 21/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.8803 - acc: 0.8470 - val_loss: 0.9993 - val_acc: 0.8358\n","Epoch 22/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.8605 - acc: 0.8498 - val_loss: 0.9960 - val_acc: 0.8379\n","Epoch 23/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.8509 - acc: 0.8505 - val_loss: 0.9900 - val_acc: 0.8370\n","Epoch 24/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.8384 - acc: 0.8524 - val_loss: 0.9806 - val_acc: 0.8391\n","Epoch 25/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.8282 - acc: 0.8542 - val_loss: 0.9617 - val_acc: 0.8412\n","Epoch 26/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.8163 - acc: 0.8559 - val_loss: 0.9527 - val_acc: 0.8431\n","Epoch 27/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.8040 - acc: 0.8586 - val_loss: 0.9793 - val_acc: 0.8385\n","Epoch 28/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.7932 - acc: 0.8604 - val_loss: 0.9568 - val_acc: 0.8414\n","Epoch 29/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7838 - acc: 0.8622 - val_loss: 0.9406 - val_acc: 0.8450\n","Epoch 30/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7774 - acc: 0.8635 - val_loss: 0.9347 - val_acc: 0.8460\n","Epoch 31/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.7670 - acc: 0.8652 - val_loss: 0.9283 - val_acc: 0.8476\n","Epoch 32/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7577 - acc: 0.8668 - val_loss: 0.9313 - val_acc: 0.8480\n","Epoch 33/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7513 - acc: 0.8683 - val_loss: 0.9192 - val_acc: 0.8495\n","Epoch 34/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7405 - acc: 0.8692 - val_loss: 0.9203 - val_acc: 0.8483\n","Epoch 35/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7294 - acc: 0.8715 - val_loss: 0.9063 - val_acc: 0.8514\n","Epoch 36/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7249 - acc: 0.8724 - val_loss: 0.9010 - val_acc: 0.8517\n","Epoch 37/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7180 - acc: 0.8746 - val_loss: 0.9068 - val_acc: 0.8506\n","Epoch 38/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7052 - acc: 0.8762 - val_loss: 0.9023 - val_acc: 0.8520\n","Epoch 39/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.7040 - acc: 0.8772 - val_loss: 0.8999 - val_acc: 0.8532\n","Epoch 40/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6976 - acc: 0.8781 - val_loss: 0.8936 - val_acc: 0.8522\n","Epoch 41/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6882 - acc: 0.8792 - val_loss: 0.9039 - val_acc: 0.8526\n","Epoch 42/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6857 - acc: 0.8804 - val_loss: 0.8922 - val_acc: 0.8550\n","Epoch 43/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.6783 - acc: 0.8825 - val_loss: 0.8871 - val_acc: 0.8548\n","Epoch 44/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.6755 - acc: 0.8829 - val_loss: 0.8885 - val_acc: 0.8553\n","Epoch 45/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6678 - acc: 0.8845 - val_loss: 0.8853 - val_acc: 0.8558\n","Epoch 46/50\n","233/233 [==============================] - 9s 38ms/step - loss: 0.6633 - acc: 0.8853 - val_loss: 0.8742 - val_acc: 0.8574\n","Epoch 47/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6580 - acc: 0.8870 - val_loss: 0.8736 - val_acc: 0.8583\n","Epoch 48/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6527 - acc: 0.8879 - val_loss: 0.8698 - val_acc: 0.8586\n","Epoch 49/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6469 - acc: 0.8889 - val_loss: 0.8706 - val_acc: 0.8589\n","Epoch 50/50\n","233/233 [==============================] - 9s 37ms/step - loss: 0.6416 - acc: 0.8899 - val_loss: 0.8859 - val_acc: 0.8563\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u7dMAurKsY6G"},"source":["저자의 경우 최종 에포크에서 훈련 데이터는 88%의 정확도를, 테스트 데이터에서는 85%의 정확도를 얻었다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"nP2PSFResZBk"},"source":["### 3.seq2seq 기계 번역기 동작시키기\r\n","\r\n","seq2seq는 훈련 과정과 테스트 과정에서의 동작 방식이 다르다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 한다. 특히 디코더를 많이 수정해야 한다. 우선 테스트 과정에서의 인코더 모델을 설계한다.\r\n"]},{"cell_type":"code","metadata":{"id":"cP7m8lmjsZD_","executionInfo":{"status":"ok","timestamp":1610608003542,"user_tz":-540,"elapsed":1032,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 인코더\r\n","encoder_model = Model(encoder_inputs, encoder_states)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yg2vg0MhsZGI"},"source":["디코더를 설계한다.\r\n"]},{"cell_type":"code","metadata":{"id":"wK7RUWdLsZIe","executionInfo":{"status":"ok","timestamp":1610608107263,"user_tz":-540,"elapsed":736,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 디코더\r\n","# 이전 시점의 상태를 보관할 텐서\r\n","decoder_state_input_h = Input(shape = (latent_dim, ))\r\n","decoder_state_input_c = Input(shape = (latent_dim, ))\r\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n","\r\n","# 훈련 때 사용했던 임베딩 층을 재사용\r\n","dec_emb2 = dec_emb_layer(decoder_inputs)\r\n","\r\n","# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\r\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\r\n","decoder_states2 = [state_h2, state_c2]\r\n","\r\n","# 모든 시점에 대해서 단어 예측\r\n","decoder_outputs2 = decoder_dense(decoder_outputs2)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7M0cVwesZJ8"},"source":["디코더를 정의한다.\r\n"]},{"cell_type":"code","metadata":{"id":"DooU-iZ5uYBU","executionInfo":{"status":"ok","timestamp":1610608155349,"user_tz":-540,"elapsed":752,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["decoder_model = Model([decoder_inputs] + decoder_states_inputs,\r\n","                      [decoder_outputs2] + decoder_states2)\r\n"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pdPq1eauhez"},"source":["테스트 과정을 위한 모델 설계를 완료하였다. 이제 테스트 과정에서의 동작을 위한 decode_sequence 함수를 구현한다.\r\n"]},{"cell_type":"code","metadata":{"id":"r0C2K0wHuoUE","executionInfo":{"status":"ok","timestamp":1610609036880,"user_tz":-540,"elapsed":510,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def decode_sequence(input_seq):\r\n","    # 입력으로부터 인코더의 상태를 얻음\r\n","    states_value = encoder_model.predict(input_seq)\r\n","\r\n","    # <SOS>에 해당하는 정수 생성\r\n","    target_seq = np.zeros((1, 1))\r\n","    target_seq[0, 0] = tar_to_index['<sos>']\r\n","\r\n","    stop_condition = False\r\n","    decoded_sentence = ''\r\n","\r\n","    # stop_condition이 True가 될 때까지 루프 반복\r\n","    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정한다.\r\n","    while not stop_condition:\r\n","        # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\r\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\r\n","\r\n","        # 예측 결과를 단어로 변환\r\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n","        sampled_char = index_to_tar[sampled_token_index]\r\n","\r\n","        # 현재 시점의 예측 단어를 예측 문장에 추가\r\n","        decoded_sentence += ' ' + sampled_char\r\n","\r\n","        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\r\n","        if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\r\n","            stop_condition = True\r\n","\r\n","        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\r\n","        target_seq = np.zeros((1, 1))\r\n","        target_seq[0, 0] = sampled_token_index\r\n","\r\n","        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\r\n","        states_value = [h, c]\r\n","\r\n","    return decoded_sentence"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8119UBPzupBo"},"source":["결과 확인을 위한 함수를 만든다."]},{"cell_type":"code","metadata":{"id":"VaTe0rGNupD6","executionInfo":{"status":"ok","timestamp":1610609038411,"user_tz":-540,"elapsed":833,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\r\n","def seq2src(input_seq):\r\n","    temp = ''\r\n","    for i in input_seq:\r\n","        if (i != 0):\r\n","            temp = temp + index_to_src[i] + ' '\r\n","    return temp\r\n","\r\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\r\n","def seq2tar(input_seq):\r\n","    temp = ''\r\n","    for i in input_seq:\r\n","        if ((i != 0 and i != tar_to_index['<sos>']) and i != tar_to_index['<eos>']):\r\n","            temp = temp + index_to_tar[i] + ' '\r\n","    return temp"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67vlNWPkupGL"},"source":["훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해보자.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEm7t5pHupJP","executionInfo":{"status":"ok","timestamp":1610609047352,"user_tz":-540,"elapsed":1661,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"c9069cc1-8f80-4b25-8a18-65a5ef87fb74"},"source":["for seq_index in [3, 50, 100, 300, 1001]:\r\n","    input_seq = encoder_input_train[seq_index: seq_index + 1]\r\n","    decoded_sentence = decode_sequence(input_seq)\r\n","\r\n","    print('원문 : ', seq2src(encoder_input_train[seq_index]))\r\n","    print('번역문 :', seq2tar(decoder_input_train[seq_index]))\r\n","    print('예측문 :', decoded_sentence[:-5])\r\n","    print('\\n')"],"execution_count":39,"outputs":[{"output_type":"stream","text":["원문 :  i told tom . \n","번역문 : j en ai parle a tom . \n","예측문 :  j ai dit que tom . \n","\n","\n","원문 :  give me the ball . \n","번역문 : donnez moi la balle ! \n","예측문 :  donne moi la main ! \n","\n","\n","원문 :  i saw tom naked . \n","번역문 : j ai vu tom nu . \n","예측문 :  j ai vu tom a vu . \n","\n","\n","원문 :  you re satisfied . \n","번역문 : vous etes content . \n","예측문 :  vous etes heureux . \n","\n","\n","원문 :  nice shot ! \n","번역문 : joli coup ! \n","예측문 :  mange tes ! \n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0JEvGfEqupPQ"},"source":["https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3"]}]}